{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vssim_loader.py\n",
    "# ===== VS Code / Jupyter 両対応：$PEDESTRIAN優先 + 無ければ24行スキップ =====\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ====== 追加: セッション跨ぎで上書きできるデフォルトパス ======\n",
    "DEFAULT_PATH = r\"C:\\Users\\yonem\\OneDrive - 学校法人立命館\\デスクトップ\\病院デフォルト\\デフォルト (8).pp\"\n",
    "\n",
    "def set_default_path(path: str):\n",
    "    \"\"\"\n",
    "    Jupyter 等から、次のセル以降へ引き継がれるデフォルトパスを設定。\n",
    "    例:\n",
    "        import vssim_loader as v\n",
    "        v.set_default_path(r\"C:\\data\\new.pp\")  # 以降のセルで有効\n",
    "        df, df_sorted, result, total, file_path = v.main()  # 引数なしでも new.pp が使われる\n",
    "    \"\"\"\n",
    "    global DEFAULT_PATH\n",
    "    DEFAULT_PATH = str(path)\n",
    "\n",
    "def _effective_default_path():\n",
    "    \"\"\"\n",
    "    優先度: 引数(--path) > 環境変数 VSSIM_DEFAULT_PATH > モジュール変数 DEFAULT_PATH\n",
    "    （※ 引数は argparse 側で処理するので、ここでは環境変数とモジュール変数のみ）\n",
    "    \"\"\"\n",
    "    return os.environ.get(\"VSSIM_DEFAULT_PATH\", DEFAULT_PATH)\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "def load_table_autodetect(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    VISSIM系の pp / txt / csv などを自動読込。\n",
    "    - $PEDESTRIAN: セクションがあれば最優先でパース\n",
    "    - それが無ければ 先頭24行スキップ＋区切り推定で読込\n",
    "    - エンコーディングは utf-8 → cp932 → utf-8-sig を順に試行\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"ファイルが見つかりません: {p}\")\n",
    "\n",
    "    # --- エンコーディング判定しつつ全文取得 ---\n",
    "    enc_used, text = None, None\n",
    "    for enc in (\"utf-8\", \"cp932\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            text = p.read_text(encoding=enc)\n",
    "            enc_used = enc\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if text is None:\n",
    "        raise ValueError(f\"ファイルを読み取れません（エンコーディング判定失敗）: {p}\")\n",
    "\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # ------------- 1) $PEDESTRIAN: を優先 -------------\n",
    "    ped_idx = None\n",
    "    for i, ln in enumerate(lines):\n",
    "        if ln.strip().upper().startswith(\"$PEDESTRIAN:\"):\n",
    "            ped_idx = i\n",
    "            break\n",
    "\n",
    "    def is_comment_or_blank(ln: str) -> bool:\n",
    "        s = ln.strip()\n",
    "        return (not s) or s.startswith(\"*\")\n",
    "\n",
    "    if ped_idx is not None:\n",
    "        header_line = lines[ped_idx].strip()\n",
    "        header_after_colon = header_line.split(\":\", 1)[1] if \":\" in header_line else header_line\n",
    "\n",
    "        # データ行収集（コメント・空行はスキップ、次のセクション開始で打ち切り）\n",
    "        data_lines = []\n",
    "        for ln in lines[ped_idx + 1:]:\n",
    "            if is_comment_or_blank(ln):\n",
    "                continue\n",
    "            if ln.strip().startswith(\"$\"):\n",
    "                break\n",
    "            data_lines.append(ln)\n",
    "\n",
    "        if not data_lines:\n",
    "            raise ValueError(\"データ行が見つかりません（$PEDESTRIAN: の後に有効な行がありません）。\")\n",
    "\n",
    "        pseudo_csv = header_after_colon + \"\\n\" + \"\\n\".join(data_lines)\n",
    "        buf = StringIO(pseudo_csv)\n",
    "        try:\n",
    "            df_ = pd.read_csv(buf, sep=\";\", engine=\"python\", on_bad_lines=\"error\")\n",
    "            used_sep = \";\"\n",
    "        except Exception:\n",
    "            buf.seek(0)\n",
    "            df_ = pd.read_csv(buf, sep=r\"[;\\t,]+\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "            used_sep = r\"[;\\t,]+\"\n",
    "\n",
    "        # 列名整形 & 互換\n",
    "        df_.columns = [c.strip() for c in df_.columns]\n",
    "        if \"PEDESTRIAN_NO\" not in df_.columns and \"NO\" in df_.columns:\n",
    "            df_ = df_.rename(columns={\"NO\": \"PEDESTRIAN_NO\"})\n",
    "\n",
    "        print(f\"検出: encoding={enc_used}, section='$PEDESTRIAN', sep={repr(used_sep)}\")\n",
    "        return df_\n",
    "\n",
    "    # ------------- 2) フォールバック：旧形式（先頭24行スキップ） -------------\n",
    "    if len(lines) <= 24:\n",
    "        raise ValueError(\"ファイルの行数が24行以下のため、削除後に読み込むデータがありません。\")\n",
    "    clean_lines = lines[24:]\n",
    "\n",
    "    # 区切り文字を推定（; / \\t / , の出現数）\n",
    "    header_line = clean_lines[0]\n",
    "    seps = [\";\", \"\\t\", \",\"]\n",
    "    sep = max(seps, key=lambda x: header_line.count(x))\n",
    "    if header_line.count(sep) == 0:\n",
    "        sep = r\"\\s+\"\n",
    "\n",
    "    print(f\"[fallback] $PEDESTRIAN: なし -> encoding={enc_used}, skipped_first_lines=24, sep={repr(sep)}\")\n",
    "    buf = StringIO(\"\\n\".join(clean_lines))\n",
    "    try:\n",
    "        df_ = pd.read_csv(buf, sep=sep, engine=\"python\", on_bad_lines=\"error\")\n",
    "    except Exception as e:\n",
    "        print(f\"[info] 一回目失敗、フォールバックします: {e}\")\n",
    "        buf.seek(0)\n",
    "        df_ = pd.read_csv(buf, sep=r\"[;\\t,]+\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "    df_.columns = [c.strip() for c in df_.columns]\n",
    "    if \"PEDESTRIAN_NO\" not in df_.columns and \"NO\" in df_.columns:\n",
    "        df_ = df_.rename(columns={\"NO\": \"PEDESTRIAN_NO\"})\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== フィルタ：MOTIONSTATE が 'Walking on level' の行だけ残す =====\n",
    "from typing import Sequence\n",
    "import pandas as pd\n",
    "\n",
    "def filter_walking_on_level(\n",
    "    df: pd.DataFrame,\n",
    "    target: str = \"Walking on level\",\n",
    "    state_col_candidates: Sequence[str] = (\"MOTIONSTATE\", \"MOTION_STATE\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DataFrame を MOTIONSTATE (候補: 'MOTIONSTATE' / 'MOTION_STATE') が target の行のみに絞る。\n",
    "    一致判定は文字列化＆strip の上で完全一致。\n",
    "    - 必須列が無い場合は KeyError\n",
    "    - 結果が空の場合は Warning を出しつつ空 DataFrame を返す（後段の処理で自然に失敗/スキップ判定できるように）\n",
    "\n",
    "    返り値: フィルタ後の DataFrame (copy)\n",
    "    \"\"\"\n",
    "    state_col = next((c for c in state_col_candidates if c in df.columns), None)\n",
    "    if state_col is None:\n",
    "        raise KeyError(f\"必要な列が見つかりません: {list(state_col_candidates)}\")\n",
    "\n",
    "    mask = df[state_col].astype(str).str.strip() == str(target).strip()\n",
    "    filtered = df.loc[mask].copy()\n",
    "\n",
    "    if filtered.empty:\n",
    "        print(f\"[WARN] '{state_col}' == '{target}' に一致する行がありません（行数=0）\")\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c070c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= 旅行時間算出ユーティリティ =========\n",
    "def compute_travel_time(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    各歩行者の DISTTRAVTOT の最小/最大行から SIMSEC を取り、(max - min) を TRAVEL_TIME として返す。\n",
    "    返り値: (df_sorted, result, total_travel_time_sum)\n",
    "    \"\"\"\n",
    "    # display があれば使う（Jupyter）、無ければ print にフォールバック\n",
    "    try:\n",
    "        from IPython.display import display as _display\n",
    "    except Exception:\n",
    "        _display = None\n",
    "\n",
    "    # 列名の自動判定（互換）\n",
    "    ped_col_candidates  = ['PEDESTRIAN:NO', 'PEDESTRIAN_NO']\n",
    "    dist_col_candidates = ['DISTTRAVTOT', 'DIST_TRAV_TOT']\n",
    "    time_col = 'SIMSEC'\n",
    "\n",
    "    ped_col  = next((c for c in ped_col_candidates  if c in df.columns), None)\n",
    "    dist_col = next((c for c in dist_col_candidates if c in df.columns), None)\n",
    "\n",
    "    missing = []\n",
    "    if ped_col is None:\n",
    "        missing.append(f\"{ped_col_candidates}\")\n",
    "    if dist_col is None:\n",
    "        missing.append(f\"{dist_col_candidates}\")\n",
    "    if time_col not in df.columns:\n",
    "        missing.append(\"['SIMSEC']\")\n",
    "    if missing:\n",
    "        raise KeyError(f\"必要な列が見つかりません: {', '.join(missing)}\")\n",
    "\n",
    "    # 1) 並び替え（歩行者→距離 昇順）\n",
    "    df_sorted = df.sort_values([ped_col, dist_col], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    # 2) 各歩行者グループで距離の最小/最大の行インデックス\n",
    "    idx_min = df_sorted.groupby(ped_col, observed=True)[dist_col].idxmin()\n",
    "    idx_max = df_sorted.groupby(ped_col, observed=True)[dist_col].idxmax()\n",
    "\n",
    "    # 3) 最小/最大行から SIMSEC を取得\n",
    "    min_rows = (df_sorted\n",
    "                .loc[idx_min, [ped_col, dist_col, time_col]]\n",
    "                .rename(columns={dist_col: 'DIST_MIN', time_col: 'SIMSEC_MIN'}))\n",
    "\n",
    "    max_rows = (df_sorted\n",
    "                .loc[idx_max, [ped_col, dist_col, time_col]]\n",
    "                .rename(columns={dist_col: 'DIST_MAX', time_col: 'SIMSEC_MAX'}))\n",
    "\n",
    "    # 4) マージして TRAVEL_TIME を算出\n",
    "    result = (min_rows\n",
    "              .merge(max_rows, on=ped_col, how='inner')\n",
    "              .assign(TRAVEL_TIME=lambda d: d['SIMSEC_MAX'] - d['SIMSEC_MIN'])\n",
    "              .sort_values(ped_col)\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    total_travel_time_sum = result['TRAVEL_TIME'].sum()\n",
    "\n",
    "    # 表示（環境に応じて）\n",
    "    print(\"並び替え済みデータ（先頭）:\")\n",
    "    if _display:\n",
    "        _display(df_sorted.head())\n",
    "    else:\n",
    "        print(df_sorted.head().to_string(index=False))\n",
    "\n",
    "    print(\"各歩行者の最小/最大行と合計旅行時間:\")\n",
    "    if _display:\n",
    "        _display(result)\n",
    "    else:\n",
    "        print(result.to_string(index=False))\n",
    "\n",
    "    print(\"全歩行者の合計（TRAVEL_TIME の総和）:\", total_travel_time_sum)\n",
    "\n",
    "    return df_sorted, result, total_travel_time_sum\n",
    "\n",
    "# ========= 追加：保存ヘルパー（まとめて保存したいときに呼ぶ） =========\n",
    "def save_parsed_df(df: pd.DataFrame, file_path: str | Path, outdir: str | Path | None = None) -> Path:\n",
    "    \"\"\"\n",
    "    df を CSV 保存。outdir を指定しなければ、元ファイルと同じ場所に *.parsed.csv を作る。\n",
    "    \"\"\"\n",
    "    src = Path(file_path)\n",
    "    if outdir is None:\n",
    "        out_path = src.with_suffix(\".parsed.csv\")\n",
    "    else:\n",
    "        outdir = Path(outdir)\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = outdir / src.with_suffix(\".parsed.csv\").name\n",
    "\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[保存] {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# ========= エントリポイント（値を返す＝保持できるように修正） =========\n",
    "def main(argv=None):\n",
    "    \"\"\"\n",
    "    VS Code/ターミナル/ Jupyter いずれからでも利用可。\n",
    "    - 値を返すので、呼び出し側で変数として保持できる\n",
    "    - 既定では保存しない（必要なら save_parsed_df を呼ぶ）\n",
    "    返り値: (df, df_sorted, result, total_travel_time_sum, file_path)\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"VISSIM系出力の自動読込（$PEDESTRIAN優先 / 無ければ24行スキップ）\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--path\", \"-p\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"入力ファイルパス（--path 未指定時は 環境変数→モジュール既定を使用）\"\n",
    "    )\n",
    "\n",
    "    # 未知の引数を無視（Jupyter の --f=... 対策）\n",
    "    args, _unknown = parser.parse_known_args(args=argv)\n",
    "\n",
    "    # 優先度: 引数 > 環境変数 > モジュール変数\n",
    "    if args.path and args.path.strip():\n",
    "        file_path = args.path\n",
    "        source = \"--path\"\n",
    "    else:\n",
    "        env_or_default = _effective_default_path()\n",
    "        file_path = env_or_default\n",
    "        source = \"env:VSSIM_DEFAULT_PATH\" if os.environ.get(\"VSSIM_DEFAULT_PATH\") else \"module:DEFAULT_PATH\"\n",
    "\n",
    "    print(f\"[INFO] using path from {source}: {file_path}\")\n",
    "\n",
    "    # --- 読み込み ---\n",
    "    try:\n",
    "        df = load_table_autodetect(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 読み込みに失敗しました: {e}\", file=sys.stderr)\n",
    "        # 保存や後続処理ができるよう、例外を再送出せず None を返す\n",
    "        return None, None, None, None, file_path\n",
    "\n",
    "    # --- 簡易サマリ ---\n",
    "    cols = list(df.columns)\n",
    "    print(\"列名(先頭30):\", cols[:30])\n",
    "    print(\"\\n----- 先頭5行 -----\")\n",
    "    try:\n",
    "        from IPython.display import display as _display\n",
    "        _display(df.head())\n",
    "    except Exception:\n",
    "        print(df.head().to_string(index=False))\n",
    "\n",
    "    # --- 旅行時間 ---\n",
    "    try:\n",
    "        df_sorted, result, total = compute_travel_time(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] 旅行時間算出に失敗: {e}\", file=sys.stderr)\n",
    "        return df, None, None, None, file_path\n",
    "\n",
    "    # ここでは保存しない。呼び出し側で必要なら save_parsed_df を呼ぶ。\n",
    "    return df, df_sorted, result, total, file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # スクリプトとして実行された場合でも、値は作って返す\n",
    "    df, df_sorted, result, total, file_path = main()\n",
    "    # 保存はデフォルトでは行わない。必要なら下のコメントアウトを外す。\n",
    "    # if df is not None:\n",
    "    #     save_parsed_df(df, file_path)  # 同じフォルダに *.parsed.csv を出力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === セル3：セル1で読み込んだ df / result を使って\n",
    "#      「AGENT_TYPE × STAROUTDECNO ごとの平均 TRAVEL_TIME」を集計（print抑止フラグ付き） ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def run_agent_star_table(df: pd.DataFrame, result: pd.DataFrame, verbose: bool = True):\n",
    "    # ---- 列名判定・検証（df / result はセル1で作成済みを想定）----\n",
    "    ped_col = next((c for c in ['PEDESTRIAN:NO','PEDESTRIAN_NO']\n",
    "                    if (c in df.columns) or (c in result.columns)), None)\n",
    "    if ped_col is None:\n",
    "        raise KeyError(\"歩行者ID列が見つかりません（PEDESTRIAN:NO / PEDESTRIAN_NO）。\")\n",
    "    if 'TRAVEL_TIME' not in result.columns:\n",
    "        raise KeyError(\"result に必須列がありません: TRAVEL_TIME\")\n",
    "    if 'PEDTYPE' not in df.columns:\n",
    "        raise KeyError(\"df に PEDTYPE 列がありません。\")\n",
    "    if 'STAROUTDECNO' not in df.columns:\n",
    "        raise KeyError(\"df に STAROUTDECNO 列がありません。\")\n",
    "\n",
    "    # 数値化（安全側）\n",
    "    df = df.copy()\n",
    "    df['PEDTYPE'] = pd.to_numeric(df['PEDTYPE'], errors='coerce')\n",
    "    df['STAROUTDECNO'] = pd.to_numeric(df['STAROUTDECNO'], errors='coerce')\n",
    "    result = result.copy()\n",
    "    result['TRAVEL_TIME'] = pd.to_numeric(result['TRAVEL_TIME'], errors='coerce')\n",
    "\n",
    "    # 代表値：最頻値→無ければ先頭（NaN除外）\n",
    "    def _mode_or_first(s: pd.Series):\n",
    "        s = s.dropna()\n",
    "        if s.empty:\n",
    "            return np.nan\n",
    "        m = s.mode(dropna=True)\n",
    "        return m.iloc[0] if len(m) > 0 else s.iloc[0]\n",
    "\n",
    "    per_ped_meta = (df.groupby(ped_col, observed=True)\n",
    "                      .agg(PEDTYPE_RAW=('PEDTYPE', _mode_or_first),\n",
    "                           STAROUTDECNO=('STAROUTDECNO', _mode_or_first))\n",
    "                      .reset_index())\n",
    "\n",
    "    # AGENT_TYPE マッピング\n",
    "    def _agent_map(x) -> str:\n",
    "        try:\n",
    "            xi = int(x)\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "        if xi in (100, 200): return 'Human'\n",
    "        if xi == 300:        return 'Robot'\n",
    "        return 'Other'\n",
    "    per_ped_meta['AGENT_TYPE'] = per_ped_meta['PEDTYPE_RAW'].apply(_agent_map)\n",
    "\n",
    "    # 結合（歩行者単位のメタを result に付与）\n",
    "    result_with_meta = result.merge(\n",
    "        per_ped_meta[[ped_col, 'AGENT_TYPE', 'STAROUTDECNO']],\n",
    "        on=ped_col, how='left'\n",
    "    )\n",
    "\n",
    "    # 集計：AGENT_TYPE × STAROUTDECNO ごとの平均 TRAVEL_TIME\n",
    "    avg = (result_with_meta\n",
    "           .groupby(['AGENT_TYPE','STAROUTDECNO'], dropna=False)['TRAVEL_TIME']\n",
    "           .mean()\n",
    "           .reset_index())\n",
    "\n",
    "    # STAR を整数表記に（.0 しかなければ Int64）\n",
    "    if pd.api.types.is_float_dtype(avg['STAROUTDECNO']):\n",
    "        frac = (avg['STAROUTDECNO'] % 1).fillna(0)\n",
    "        if (frac == 0).all():\n",
    "            avg['STAROUTDECNO'] = avg['STAROUTDECNO'].astype('Int64')\n",
    "\n",
    "    # 並べ替え\n",
    "    avg = avg.sort_values(['AGENT_TYPE','STAROUTDECNO']).reset_index(drop=True)\n",
    "\n",
    "    # 表示（任意）\n",
    "    if verbose:\n",
    "        print(\"【AGENT_TYPE × STAROUTDECNO ごとの平均 TRAVEL_TIME】\")\n",
    "        try:\n",
    "            from IPython.display import display as _display\n",
    "            _display(avg)\n",
    "        except Exception:\n",
    "            print(avg.to_string(index=False))\n",
    "\n",
    "    return avg, result_with_meta\n",
    "\n",
    "# ★ 実行（セル1で作られた df / result をそのまま使用）\n",
    "avg_travel_by_agent_star, result_with_meta = run_agent_star_table(df, result, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751994a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AGENT_TYPE × STAROUTDECNO ごとの平均 TRAVEL_TIME（print抑止フラグ付き） ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def run_agent_star_table(df: pd.DataFrame, result: pd.DataFrame, verbose: bool = True):\n",
    "    # ---- 列名判定・検証 ----\n",
    "    ped_col = next((c for c in ['PEDESTRIAN:NO','PEDESTRIAN_NO'] if c in df.columns or c in result.columns), None)\n",
    "    if ped_col is None:\n",
    "        raise KeyError(\"歩行者ID列が見つかりません（PEDESTRIAN:NO / PEDESTRIAN_NO）。\")\n",
    "    if 'TRAVEL_TIME' not in result.columns:\n",
    "        raise KeyError(\"result に必須列がありません: TRAVEL_TIME\")\n",
    "    if 'PEDTYPE' not in df.columns:\n",
    "        raise KeyError(\"df に PEDTYPE 列がありません。\")\n",
    "    if 'STAROUTDECNO' not in df.columns:\n",
    "        raise KeyError(\"df に STAROUTDECNO 列がありません。\")\n",
    "\n",
    "    # 数値化\n",
    "    df = df.copy()\n",
    "    df['PEDTYPE'] = pd.to_numeric(df['PEDTYPE'], errors='coerce')\n",
    "    df['STAROUTDECNO'] = pd.to_numeric(df['STAROUTDECNO'], errors='coerce')\n",
    "    result = result.copy()\n",
    "    result['TRAVEL_TIME'] = pd.to_numeric(result['TRAVEL_TIME'], errors='coerce')\n",
    "\n",
    "    # 代表値：最頻値→無ければ先頭（NaN除外）\n",
    "    def _mode_or_first(s: pd.Series):\n",
    "        s = s.dropna()\n",
    "        if s.empty:\n",
    "            return np.nan\n",
    "        m = s.mode(dropna=True)\n",
    "        return m.iloc[0] if len(m) > 0 else s.iloc[0]\n",
    "\n",
    "    per_ped_meta = (df.groupby(ped_col, observed=True)\n",
    "                      .agg(PEDTYPE_RAW=('PEDTYPE', _mode_or_first),\n",
    "                           STAROUTDECNO=('STAROUTDECNO', _mode_or_first))\n",
    "                      .reset_index())\n",
    "\n",
    "    # AGENT_TYPE マッピング\n",
    "    def _agent_map(x) -> str:\n",
    "        try:\n",
    "            xi = int(x)\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "        if xi in (100, 200): return 'Human'\n",
    "        if xi == 300:        return 'Robot'\n",
    "        return 'Other'\n",
    "    per_ped_meta['AGENT_TYPE'] = per_ped_meta['PEDTYPE_RAW'].apply(_agent_map)\n",
    "\n",
    "    # 結合\n",
    "    result_with_meta = result.merge(per_ped_meta[[ped_col, 'AGENT_TYPE', 'STAROUTDECNO']],\n",
    "                                    on=ped_col, how='left')\n",
    "\n",
    "    # 集計\n",
    "    avg = (result_with_meta\n",
    "           .groupby(['AGENT_TYPE','STAROUTDECNO'], dropna=False)['TRAVEL_TIME']\n",
    "           .mean()\n",
    "           .reset_index())\n",
    "\n",
    "    # STAR を整数表記に（.0 しかなければ Int64）\n",
    "    if pd.api.types.is_float_dtype(avg['STAROUTDECNO']):\n",
    "        frac = (avg['STAROUTDECNO'] % 1).fillna(0)\n",
    "        if (frac == 0).all():\n",
    "            avg['STAROUTDECNO'] = avg['STAROUTDECNO'].astype('Int64')\n",
    "\n",
    "    # 並べ替え\n",
    "    avg = avg.sort_values(['AGENT_TYPE','STAROUTDECNO']).reset_index(drop=True)\n",
    "\n",
    "    # 表示（任意）\n",
    "    if verbose:\n",
    "        print(\"【AGENT_TYPE × STAROUTDECNO ごとの平均 TRAVEL_TIME】\")\n",
    "        print(avg.to_string(index=False))\n",
    "\n",
    "    return avg, result_with_meta\n",
    "\n",
    "# ★ 実行（df と result が既にある前提）\n",
    "avg_travel_by_agent_star, result_with_meta = run_agent_star_table(df, result, verbose=True)  # ←表示したくなければ False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 速度分散 第一処理（堅牢版・Jupyter/ターミナル両対応） ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# display が使える環境なら使う、なければ print にフォールバック\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "def _show(df, head=20, title=None):\n",
    "    if title: print(title)\n",
    "    if _display:\n",
    "        _display(df.head(head))\n",
    "    else:\n",
    "        print(df.head(head).to_string(index=False))\n",
    "\n",
    "# --- df の存在確認 ---\n",
    "if \"df\" not in globals() or not isinstance(df, pd.DataFrame):\n",
    "    raise RuntimeError(\"df が見つかりません。先に読み込みセルを実行してください。\")\n",
    "\n",
    "work = df.copy()\n",
    "\n",
    "# === 0) 列名の正規化と基本整形 ===\n",
    "# 時刻\n",
    "time_col = (\n",
    "    'TIME_SEC' if 'TIME_SEC' in work.columns else\n",
    "    ('SIMSEC' if 'SIMSEC' in work.columns else None)\n",
    ")\n",
    "# 歩行者ID（コロン・アンダースコア両対応。誤って $ が付いた列名にも一応対応）\n",
    "ped_col_candidates = ['PEDESTRIAN_NO', 'PEDESTRIAN:NO', '$PEDESTRIAN:NO']\n",
    "ped_col = next((c for c in ped_col_candidates if c in work.columns), None)\n",
    "\n",
    "# 分散列（表記ゆれ両対応）\n",
    "var_col = (\n",
    "    'EXPER_VEL_VAR' if 'EXPER_VEL_VAR' in work.columns else\n",
    "    ('EXPERVELVAR' if 'EXPERVELVAR' in work.columns else None)\n",
    ")\n",
    "\n",
    "need_names = {'time': time_col, 'ped': ped_col, 'var': var_col}\n",
    "missing = [k for k, v in need_names.items() if v is None or v not in work.columns]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"必要列が見つかりません: {missing}  \"\n",
    "        f\"(TIME_SEC / SIMSEC, PEDESTRIAN_NO / PEDESTRIAN:NO, EXPER_VEL_VAR / EXPERVELVAR を確認)\"\n",
    "    )\n",
    "\n",
    "# === ここから置き換え：COORD → X/Y/Z を確実に用意 ===\n",
    "def ensure_xyz(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "\n",
    "    # 既に X/Y/Z があれば数値化して返す\n",
    "    if {'X','Y','Z'}.issubset(df_out.columns):\n",
    "        for c in ['X','Y','Z']:\n",
    "            df_out[c] = pd.to_numeric(df_out[c], errors='coerce')\n",
    "        return df_out\n",
    "\n",
    "    # 1) 既知の候補名を優先\n",
    "    known_names = {'COORD_CENT','COORDCENT','COORD_CENTER','COORDCENTER','COORD'}\n",
    "    cand = [c for c in df_out.columns if c.upper() in known_names]\n",
    "\n",
    "    # 2) 見つからなければ object 列を広めに候補化\n",
    "    if not cand:\n",
    "        cand = [c for c in df_out.columns if df_out[c].dtype == object]\n",
    "\n",
    "    # 数値3連（空白/カンマ/セミコロン区切り）検出\n",
    "    num = r'([+-]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)'\n",
    "    pat = re.compile(r'^\\s*' + num + r'[\\s,;]+' + num + r'[\\s,;]+' + num + r'\\s*$')\n",
    "\n",
    "    best_col, best_match, best_count = None, None, -1\n",
    "    for c in cand:\n",
    "        s = df_out[c].astype(str)\n",
    "        m = s.str.extract(pat)\n",
    "        cnt = (m.notna().all(axis=1)).sum()\n",
    "        if cnt > best_count:\n",
    "            best_col, best_match, best_count = c, m, cnt\n",
    "\n",
    "    if best_col is not None and best_count > 0:\n",
    "        best_match.columns = ['X','Y','Z']\n",
    "        for col in ['X','Y','Z']:\n",
    "            df_out[col] = pd.to_numeric(best_match[col], errors='coerce')\n",
    "    else:\n",
    "        for col in ['X','Y','Z']:\n",
    "            if col not in df_out.columns:\n",
    "                df_out[col] = np.nan\n",
    "\n",
    "    return df_out\n",
    "\n",
    "work = ensure_xyz(work)\n",
    "# === 置き換えここまで ===\n",
    "\n",
    "# 数値化（速度は VISSIM の列名ゆらぎも面倒見る：SPEED / DES_SPEED / DESSPEED）\n",
    "numeric_candidates = [time_col, var_col, 'X','Y','Z','SPEED','DES_SPEED','DESSPEED']\n",
    "for c in numeric_candidates:\n",
    "    if c in work.columns:\n",
    "        work[c] = pd.to_numeric(work[c], errors='coerce')\n",
    "\n",
    "# 並び替え（歩行者→時刻）\n",
    "work.sort_values([ped_col, time_col], inplace=True, kind='mergesort')\n",
    "\n",
    "# 衝突しない元行番号列名を作成\n",
    "orig_idx_col = '__ROW_ORIG__'\n",
    "while orig_idx_col in work.columns:\n",
    "    orig_idx_col += '_X'\n",
    "\n",
    "# 元 index を保存（重複回避版）\n",
    "work.reset_index(drop=False, inplace=True)\n",
    "work.rename(columns={'index': orig_idx_col}, inplace=True)\n",
    "\n",
    "# === 1) しきい値（分散 σv^2 ベース） ===\n",
    "bins_var = [-np.inf, 0.02, 0.25, 1.0, 4.0, 9.0, np.inf]\n",
    "labels = [\n",
    "    '等速（ゼロ帯）',     # ≤0.02\n",
    "    '通常',               # (0.02, 0.25]\n",
    "    '軽度不安定',         # (0.25, 1.0]\n",
    "    '中等度不安定',       # (1.0, 4.0]\n",
    "    '物理限界超え疑い',   # (4.0, 9.0]\n",
    "    '非現実（異常確定）'  # >9.0\n",
    "]\n",
    "\n",
    "work['VAR'] = work[var_col]\n",
    "work['STD'] = np.sqrt(work['VAR'])\n",
    "work['EXPERVELVAR_LABEL'] = pd.cut(\n",
    "    work['VAR'], bins=bins_var, labels=labels, include_lowest=True, right=True\n",
    ")\n",
    "\n",
    "# === 2) “同ラベルが連続” を 1現象にまとめる（歩行者ごと） ===\n",
    "tmp_label = (\n",
    "    work['EXPERVELVAR_LABEL']\n",
    "      .astype('object')\n",
    "      .where(work['EXPERVELVAR_LABEL'].notna(), '（未分類）')\n",
    ")\n",
    "\n",
    "work['_label_block_id'] = (\n",
    "    tmp_label\n",
    "    .groupby(work[ped_col])                       # Series の groupby\n",
    "    .apply(lambda s: (s != s.shift()).cumsum())\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# === 3) エピソード（連続区間）要約 ===\n",
    "def trend_label(dv, eps=0.05):\n",
    "    if pd.isna(dv): return '不明'\n",
    "    if dv >  eps:   return '速度増加'\n",
    "    if dv < -eps:   return '速度低下'\n",
    "    return '概ね横ばい'\n",
    "\n",
    "def safe_get(series, pos):\n",
    "    try:\n",
    "        return series.iloc[pos]\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "episodes_rows = []\n",
    "for (pid, blk), g in work.groupby([ped_col, '_label_block_id'], sort=True):\n",
    "    lab = g['EXPERVELVAR_LABEL'].iloc[0]\n",
    "    lab_str = lab if pd.notna(lab) else '（未分類）'\n",
    "    t0 = g[time_col].iloc[0]; t1 = g[time_col].iloc[-1]\n",
    "    n  = len(g)\n",
    "\n",
    "    # 速度トレンド（列名ゆらぎ対応：DES_SPEED or DESSPEED は参考値）\n",
    "    speed0 = g['SPEED'].iloc[0] if 'SPEED' in g.columns else np.nan\n",
    "    speed1 = g['SPEED'].iloc[-1] if 'SPEED' in g.columns else np.nan\n",
    "    d_speed = (speed1 - speed0) if (pd.notna(speed0) and pd.notna(speed1)) else np.nan\n",
    "    tr = trend_label(d_speed)\n",
    "\n",
    "    # 座標\n",
    "    x0, y0, z0 = safe_get(g['X'],0),  safe_get(g['Y'],0),  safe_get(g['Z'],0)\n",
    "    x1, y1, z1 = safe_get(g['X'],-1), safe_get(g['Y'],-1), safe_get(g['Z'],-1)\n",
    "    x_med = g['X'].median() if 'X' in g.columns else np.nan\n",
    "    y_med = g['Y'].median() if 'Y' in g.columns else np.nan\n",
    "    z_med = g['Z'].median() if 'Z' in g.columns else np.nan\n",
    "\n",
    "    # 区間中の分散/標準偏差指標\n",
    "    var_max = g['VAR'].max()\n",
    "    var_med = g['VAR'].median()\n",
    "    std_med = g['STD'].median()\n",
    "\n",
    "    # 元行位置（nullable int へ）\n",
    "    row_start_val = g[orig_idx_col].iloc[0]\n",
    "    row_end_val   = g[orig_idx_col].iloc[-1]\n",
    "    row_start = pd.to_numeric(pd.Series([row_start_val]), errors='coerce').iloc[0]\n",
    "    row_end   = pd.to_numeric(pd.Series([row_end_val]),   errors='coerce').iloc[0]\n",
    "\n",
    "    episodes_rows.append({\n",
    "        ped_col: pid,\n",
    "        'BLOCK_ID': int(blk),\n",
    "        'LABEL': lab_str,\n",
    "        'N_SAMPLES': n,\n",
    "        'T_START': t0,\n",
    "        'T_END': t1,\n",
    "        'DURATION': (t1 - t0) if (pd.notna(t0) and pd.notna(t1)) else np.nan,\n",
    "        'VAR_MED': var_med,\n",
    "        'VAR_MAX': var_max,\n",
    "        'STD_MED': std_med,\n",
    "        'SPEED_START': speed0,\n",
    "        'SPEED_END': speed1,\n",
    "        'DELTA_SPEED': d_speed,\n",
    "        'SPEED_TREND': tr,\n",
    "        'X_START': x0, 'Y_START': y0, 'Z_START': z0,\n",
    "        'X_END':   x1, 'Y_END':   y1, 'Z_END':   z1,\n",
    "        'X_MED': x_med, 'Y_MED': y_med, 'Z_MED': z_med,\n",
    "        'ROW_START': row_start,\n",
    "        'ROW_END':   row_end,\n",
    "    })\n",
    "\n",
    "episodes = (\n",
    "    pd.DataFrame(episodes_rows)\n",
    "      .sort_values([ped_col, 'T_START', 'BLOCK_ID'])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ROW_* は nullable 整数に（NaN を許容）\n",
    "for c in ['ROW_START','ROW_END']:\n",
    "    if c in episodes.columns:\n",
    "        episodes[c] = episodes[c].astype('Int64')\n",
    "\n",
    "# === 4) ダッシュボード的な集計 ===\n",
    "abnormal_levels = ['中等度不安定','物理限界超え疑い','非現実（異常確定）']\n",
    "abnormal = episodes[episodes['LABEL'].isin(abnormal_levels)]\n",
    "\n",
    "count_table = (\n",
    "    episodes\n",
    "    .groupby([ped_col, 'LABEL'], dropna=False)\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 出力\n",
    "_show(episodes, head=20, title=\"=== episodes（先頭）===\")\n",
    "_show(count_table, head=20, title=\"=== 歩行者×ラベル件数（先頭）===\")\n",
    "\n",
    "# === 5) CSV保存（Excel互換の utf-8-sig） ===\n",
    "#episodes.to_csv('expvelvar_episodes.csv', index=False, encoding='utf-8-sig')\n",
    "#count_table.to_csv('expvelvar_counts_by_ped.csv', index=False, encoding='utf-8-sig')\n",
    "#abnormal.to_csv('expvelvar_abnormal_only.csv', index=False, encoding='utf-8-sig')\n",
    "#print(\"保存: expvelvar_episodes.csv / expvelvar_counts_by_ped.csv / expvelvar_abnormal_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abdca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 速度分散 第二処理（堅牢版・Jupyter/ターミナル両対応・保存しない版） ===\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "\n",
    "def _show(df, title):\n",
    "    print(title)\n",
    "    if _display: _display(df)\n",
    "    else: print(df.to_string())\n",
    "\n",
    "def run_expvelvar_second(episodes: pd.DataFrame):\n",
    "    # 1) 前提チェック\n",
    "    if 'episodes' not in globals() or not isinstance(episodes, pd.DataFrame):\n",
    "        raise RuntimeError(\"先にエピソード抽出セル（第一処理）を実行して、episodes を作ってください。\")\n",
    "    if 'LABEL' not in episodes.columns:\n",
    "        raise KeyError(\"episodes に 'LABEL' 列が見つかりません。第一処理の結果を確認してください。\")\n",
    "\n",
    "    # 2) 歩行者ID列の特定\n",
    "    ped_candidates = ['PEDESTRIAN:NO', 'PEDESTRIAN_NO']\n",
    "    ped_col = next((c for c in ped_candidates if c in episodes.columns), None)\n",
    "    if ped_col is None:\n",
    "        known_non_id = {\n",
    "            'BLOCK_ID','LABEL','N_SAMPLES','T_START','T_END','DURATION',\n",
    "            'VAR_MED','VAR_MAX','STD_MED','SPEED_START','SPEED_END','DELTA_SPEED','SPEED_TREND',\n",
    "            'X_START','Y_START','Z_START','X_END','Y_END','Z_END','X_MED','Y_MED','Z_MED','ROW_START','ROW_END'\n",
    "        }\n",
    "        candidates = [c for c in episodes.columns if c not in known_non_id]\n",
    "        ped_col = candidates[0] if candidates else episodes.columns[0]\n",
    "\n",
    "    # 3) ラベル順\n",
    "    base_order = [\n",
    "        '等速（ゼロ帯）','通常','軽度不安定','中等度不安定','物理限界超え疑い','非現実（異常確定）','（未分類）'\n",
    "    ]\n",
    "    existing = episodes['LABEL'].dropna().astype(str).unique().tolist()\n",
    "    tail = [l for l in existing if l not in base_order]\n",
    "    level_order = base_order + tail\n",
    "    labels_cat = pd.Categorical(episodes['LABEL'].astype('object'), categories=level_order, ordered=True)\n",
    "\n",
    "    # 4) レベル別総件数\n",
    "    level_counts = (\n",
    "        pd.Series(labels_cat, name='EXPERVELVAR_LEVEL')\n",
    "          .value_counts(dropna=False)\n",
    "          .sort_index()\n",
    "          .reset_index(name='COUNT')\n",
    "          .rename(columns={'index': 'EXPERVELVAR_LEVEL'})\n",
    "    )\n",
    "    _show(level_counts, \"=== EXPERVELVAR レベル別の発生回数 ===\")\n",
    "\n",
    "    # 5) 歩行者 × レベル 件数\n",
    "    level_by_ped = (\n",
    "        episodes.assign(_LABEL=labels_cat)\n",
    "                .groupby([ped_col, '_LABEL'], dropna=False)\n",
    "                .size()\n",
    "                .unstack('_LABEL', fill_value=0)\n",
    "                .reindex(columns=level_order, fill_value=0)\n",
    "    )\n",
    "    try: level_by_ped = level_by_ped.sort_index()\n",
    "    except Exception: pass\n",
    "    _show(level_by_ped, \"=== 歩行者別 × レベル別 件数 ===\")\n",
    "\n",
    "    # ここでは保存しない。呼び出し側で保存。\n",
    "    return level_counts, level_by_ped\n",
    "\n",
    "# ★ 実行（episodes がある前提）\n",
    "level_counts, level_by_ped = run_expvelvar_second(episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93304357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 速度分散：総平均 / PEDTYPE 集約 / 時間ビン / STAROUTDECNO 集約（堅牢版） ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# display が使える環境なら使う、なければ print にフォールバック\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "def _show(df, title, head=None):\n",
    "    print(title)\n",
    "    if head is not None:\n",
    "        df = df.head(head)\n",
    "    if _display:\n",
    "        _display(df)\n",
    "    else:\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "# --- df の存在確認 ---\n",
    "if \"df\" not in globals() or not isinstance(df, pd.DataFrame):\n",
    "    raise RuntimeError(\"df が見つかりません。先に読み込みセルを実行してください。\")\n",
    "\n",
    "w = df.copy()\n",
    "\n",
    "# --- 必要列の特定（柔軟対応） ---\n",
    "# 速度分散列\n",
    "var_col = None\n",
    "for c in (\"EXPER_VEL_VAR\", \"EXPERVELVAR\"):\n",
    "    if c in w.columns:\n",
    "        var_col = c\n",
    "        break\n",
    "\n",
    "# 時刻列（SIMSEC を優先。無ければ TIME_SEC）\n",
    "time_col = None\n",
    "for c in (\"SIMSEC\", \"TIME_SEC\"):\n",
    "    if c in w.columns:\n",
    "        time_col = c\n",
    "        break\n",
    "\n",
    "pedtype_exists   = \"PEDTYPE\"      in w.columns\n",
    "starout_exists   = \"STAROUTDECNO\" in w.columns\n",
    "\n",
    "need = {\n",
    "    \"速度分散列 (EXPER_VEL_VAR/EXPERVELVAR)\": var_col,\n",
    "    \"時刻列 (SIMSEC/TIME_SEC)\": time_col,\n",
    "    \"PEDTYPE\": \"PEDTYPE\" if pedtype_exists else None,\n",
    "    \"STAROUTDECNO\": \"STAROUTDECNO\" if starout_exists else None,\n",
    "}\n",
    "missing = [k for k, v in need.items() if v is None]\n",
    "if missing:\n",
    "    raise ValueError(f\"必要列が見つかりません: {missing}\")\n",
    "\n",
    "# --- 数値化（安全） ---\n",
    "w[var_col] = pd.to_numeric(w[var_col], errors=\"coerce\")\n",
    "w[time_col] = pd.to_numeric(w[time_col], errors=\"coerce\")\n",
    "w[\"PEDTYPE\"] = pd.to_numeric(w[\"PEDTYPE\"], errors=\"coerce\")\n",
    "w[\"STAROUTDECNO\"] = pd.to_numeric(w[\"STAROUTDECNO\"], errors=\"coerce\")\n",
    "\n",
    "# --- PEDTYPE を 3分類にマッピング（100/200=HUMAN, 300=ROBOT, その他=OTHER） ---\n",
    "def _agent_group(x) -> str:\n",
    "    try:\n",
    "        xi = int(x)\n",
    "    except Exception:\n",
    "        return \"OTHER\"\n",
    "    if xi in (100, 200): return \"HUMAN\"\n",
    "    if xi == 300:        return \"ROBOT\"\n",
    "    return \"OTHER\"\n",
    "\n",
    "w[\"AGENT_GROUP\"] = w[\"PEDTYPE\"].apply(_agent_group)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) 全体の平均速度分散\n",
    "# ---------------------------------------------------------------\n",
    "overall_mean = w[var_col].mean(skipna=True)\n",
    "overall_df = pd.DataFrame({\"METRIC\": [\"OVERALL_MEAN_VAR\"], \"MEAN_VAR\": [overall_mean]})\n",
    "_show(overall_df, \"=== 全体の平均速度分散 ===\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2) AGENT_GROUP 別（HUMAN / ROBOT / OTHER）の平均速度分散\n",
    "# ---------------------------------------------------------------\n",
    "by_type = (\n",
    "    w.groupby(\"AGENT_GROUP\", dropna=False, as_index=False)[var_col]\n",
    "     .mean(numeric_only=True)\n",
    "     .rename(columns={var_col: \"MEAN_VAR\"})\n",
    "     .sort_values(\"AGENT_GROUP\")\n",
    "     .reset_index(drop=True)\n",
    ")\n",
    "_show(by_type, \"=== AGENT_GROUP 別 平均速度分散（HUMAN/ROBOT/OTHER） ===\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3) AGENT_GROUP 別 × 時間ビン（既定 1800 秒）の平均速度分散\n",
    "#    到着基準にしたい場合は time_col を変更してください\n",
    "# ---------------------------------------------------------------\n",
    "BIN = 1800  # 30分=1800秒\n",
    "\n",
    "# 欠損は除外しつつビンを算出\n",
    "t = pd.to_numeric(w[time_col], errors=\"coerce\")\n",
    "# NaN を一時的に -1 にして floor_divide、負値はのちに NaN に戻す\n",
    "bin_start = np.floor_divide(t.fillna(-1).astype(\"int64\"), BIN) * BIN\n",
    "bin_start = bin_start.where(bin_start >= 0, pd.NA).astype(\"Int64\")\n",
    "\n",
    "w_time = w.copy()\n",
    "w_time[\"TIME_BIN_START\"] = bin_start\n",
    "w_time = w_time[w_time[\"TIME_BIN_START\"].notna()].copy()\n",
    "\n",
    "def _sec_to_hhmmss(x) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"NaT\"\n",
    "    x = int(x)\n",
    "    h = x // 3600\n",
    "    m = (x % 3600) // 60\n",
    "    s = x % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "w_time[\"TIME_BIN_LABEL\"] = w_time[\"TIME_BIN_START\"].apply(_sec_to_hhmmss)\n",
    "\n",
    "by_type_time = (\n",
    "    w_time.groupby([\"AGENT_GROUP\", \"TIME_BIN_START\", \"TIME_BIN_LABEL\"], dropna=False, as_index=False)[var_col]\n",
    "          .mean(numeric_only=True)\n",
    "          .rename(columns={var_col: \"MEAN_VAR\"})\n",
    "          .sort_values([\"AGENT_GROUP\", \"TIME_BIN_START\", \"TIME_BIN_LABEL\"])\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "_show(by_type_time, f\"=== AGENT_GROUP 別 × 時間ビン（{BIN}秒）平均速度分散 ===\", head=30)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4) AGENT_GROUP 別 × STAROUTDECNO 別 平均速度分散\n",
    "# ---------------------------------------------------------------\n",
    "by_type_star = (\n",
    "    w.groupby([\"AGENT_GROUP\", \"STAROUTDECNO\"], dropna=False, as_index=False)[var_col]\n",
    "     .mean(numeric_only=True)\n",
    "     .rename(columns={var_col: \"MEAN_VAR\"})\n",
    "     .sort_values([\"AGENT_GROUP\", \"STAROUTDECNO\"])\n",
    "     .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# STAROUTDECNO が実数型かつ .0 なら Int64 に見せる\n",
    "if pd.api.types.is_float_dtype(by_type_star[\"STAROUTDECNO\"]):\n",
    "    frac = (by_type_star[\"STAROUTDECNO\"] % 1).fillna(0)\n",
    "    if (frac == 0).all():\n",
    "        by_type_star[\"STAROUTDECNO\"] = by_type_star[\"STAROUTDECNO\"].astype(\"Int64\")\n",
    "\n",
    "_show(by_type_star, \"=== AGENT_GROUP 別 × STAROUTDECNO 別 平均速度分散 ===\", head=30)\n",
    "\n",
    "# ---（必要ならCSV保存：Excel互換の UTF-8 BOM 付き）---\n",
    "# overall_df.to_csv('mean_var_overall.csv', index=False, encoding='utf-8-sig')\n",
    "# by_type.to_csv('mean_var_by_type.csv', index=False, encoding='utf-8-sig')\n",
    "# by_type_time.to_csv('mean_var_by_type_timebin.csv', index=False, encoding='utf-8-sig')\n",
    "# by_type_star.to_csv('mean_var_by_type_star.csv', index=False, encoding='utf-8-sig')\n",
    "# print(\"保存: mean_var_overall.csv / mean_var_by_type.csv / mean_var_by_type_timebin.csv / mean_var_by_type_star.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ea7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === イベント回数（堅牢版・Jupyter/ターミナル両対応｜保存しない版） ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata as ud\n",
    "import re\n",
    "\n",
    "# 表示ユーティリティ\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "def _show(df, title=None, head=None):\n",
    "    if title: print(title)\n",
    "    if head is not None:\n",
    "        df = df.head(head)\n",
    "    if _display: _display(df)\n",
    "    else: print(df.to_string(index=False))\n",
    "\n",
    "# --- パラメータ ---\n",
    "TOL_NEIGHBOR_SEC = 0.5\n",
    "EVENT_GAP_SEC    = 1.0\n",
    "CLUSTER_SEC      = 3.0\n",
    "DIST_LIMIT       = 0.5\n",
    "PASS_BY_DEG      = 100\n",
    "\n",
    "# --- df の存在確認 ---\n",
    "if \"df\" not in globals() or not isinstance(df, pd.DataFrame):\n",
    "    raise RuntimeError(\"df が見つかりません。先に読み込みセルを実行してください。\")\n",
    "\n",
    "# --- ユーティリティ ---\n",
    "def norm(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = ud.normalize(\"NFKC\", str(s)).strip().lower()\n",
    "    s = s.replace(\"$\",\"\").replace(\"：\",\":\").replace(\"（\",\"(\").replace(\"）\",\")\")\n",
    "    for ch in [\" \", \"\\t\", \"-\", \"/\", \"\\\\\", \":\"]:\n",
    "        s = s.replace(ch, \"_\")\n",
    "    while \"__\" in s: s = s.replace(\"__\",\"_\")\n",
    "    return s\n",
    "\n",
    "def find_col(df_: pd.DataFrame, candidates) -> str:\n",
    "    nmap = {c: norm(c) for c in df_.columns}\n",
    "    wants = {norm(c) for c in candidates}\n",
    "    for col, coln in nmap.items():\n",
    "        if coln in wants:\n",
    "            return col\n",
    "    return \"\"\n",
    "\n",
    "def to_num(s):\n",
    "    try:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "# --- 列名検出（実列名を変数に保持：リネームしない）---\n",
    "COL_ID   = find_col(df, [\"$PEDESTRIAN:NO\",\"PEDESTRIAN:NO\",\"PEDESTRIAN_NO\",\"AGENT_ID\",\"ID\",\"AgentID\"])\n",
    "COL_TYPE = find_col(df, [\"PEDTYPE\",\"TYPE\"])\n",
    "COL_ANG  = find_col(df, [\"ORIENTATIONANGLE\",\"ORIENTATION_ANGLE\",\"ANGLE\"])\n",
    "COL_T    = find_col(df, [\"SIMSEC\",\"TIME\",\"TIMESTAMP\",\"SEC\"])\n",
    "COL_NID  = find_col(df, [\"NEARNEIGHB\",\"NEAR_NEIGHB\",\"NEIGHBOR\",\"NEIGHBORID\"])\n",
    "COL_ND   = find_col(df, [\"NEARNEIGHBDIST\",\"NEAR_NEIGHB_DIST\",\"NEIGHBORDIST\",\"NEIGHBOR_DIST\"])\n",
    "COL_COORD= find_col(df, [\"COORDCENT\",\"COORD\",\"COORD_CENTER\",\"CENTROID\"])\n",
    "\n",
    "COL_X = find_col(df, [\"X\",\"XCOORD\",\"XCoord\",\"$PEDESTRIAN:X\",\"POSX\",\"X [m]\",\"X[m]\",\"x\",\"pos_x\"])\n",
    "COL_Y = find_col(df, [\"Y\",\"YCOORD\",\"YCoord\",\"$PEDESTRIAN:Y\",\"POSY\",\"Y [m]\",\"Y[m]\",\"y\",\"pos_y\"])\n",
    "COL_Z = find_col(df, [\"Z\",\"ZCOORD\",\"ZCoord\",\"$PEDESTRIAN:Z\",\"POSZ\",\"Z [m]\",\"Z[m]\",\"z\",\"pos_z\"])\n",
    "\n",
    "need = [COL_ID, COL_TYPE, COL_ANG, COL_T, COL_NID, COL_ND]\n",
    "if any(c == \"\" for c in need):\n",
    "    names = [\"ID\",\"PEDTYPE\",\"ORIENTATIONANGLE\",\"SIMSEC\",\"NEARNEIGHB\",\"NEARNEIGHBDIST\"]\n",
    "    have  = [bool(COL_ID),bool(COL_TYPE),bool(COL_ANG),bool(COL_T),bool(COL_NID),bool(COL_ND)]\n",
    "    lack  = [n for n, ok in zip(names, have) if not ok]\n",
    "    raise KeyError(f\"必須列が見つかりません: {lack}\")\n",
    "\n",
    "# --- 数値化 ---\n",
    "for c in [COL_ID, COL_TYPE, COL_ANG, COL_T, COL_NID, COL_ND, COL_X, COL_Y, COL_Z]:\n",
    "    if c:\n",
    "        df[c] = to_num(df[c])\n",
    "\n",
    "# --- COORDCENT → SelfX/Y/Z（X/Yが無い時のみ）---\n",
    "if (not COL_X or not COL_Y) and COL_COORD:\n",
    "    pat = re.compile(r\"\\s*([+-]?\\d+(?:\\.\\d+)?)\\s+([+-]?\\d+(?:\\.\\d+)?)\\s+([+-]?\\d+(?:\\.\\d+)?)\\s*\")\n",
    "    xyz = df[COL_COORD].astype(str).str.extract(pat)\n",
    "    df[\"SelfX\"] = pd.to_numeric(xyz[0], errors=\"coerce\")\n",
    "    df[\"SelfY\"] = pd.to_numeric(xyz[1], errors=\"coerce\")\n",
    "    df[\"SelfZ\"] = pd.to_numeric(xyz[2], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"SelfX\"] = pd.to_numeric(df[COL_X], errors=\"coerce\") if COL_X else np.nan\n",
    "    df[\"SelfY\"] = pd.to_numeric(df[COL_Y], errors=\"coerce\") if COL_Y else np.nan\n",
    "    df[\"SelfZ\"] = pd.to_numeric(df[COL_Z], errors=\"coerce\") if COL_Z else np.nan\n",
    "\n",
    "# --- 角度だけ asof で取得（Neighbor座標は扱わない）---\n",
    "left = df[[COL_ID, COL_T, COL_NID, COL_ND, COL_ANG, COL_TYPE, \"SelfX\",\"SelfY\",\"SelfZ\"]].copy()\n",
    "left = left.rename(columns={COL_T:\"SIMSEC\"})\n",
    "right = df[[COL_ID, COL_T, COL_ANG]].copy().rename(columns={COL_T:\"SIMSEC\"})\n",
    "\n",
    "left[\"match_id\"] = pd.to_numeric(left[COL_NID], errors=\"coerce\")\n",
    "right[\"match_id\"] = pd.to_numeric(right[COL_ID], errors=\"coerce\")\n",
    "left[\"SIMSEC\"]   = pd.to_numeric(left[\"SIMSEC\"], errors=\"coerce\")\n",
    "right[\"SIMSEC\"]  = pd.to_numeric(right[\"SIMSEC\"], errors=\"coerce\")\n",
    "left[\"NEARNEIGHBDIST\"]   = pd.to_numeric(left[COL_ND], errors=\"coerce\")\n",
    "left[\"ORIENTATIONANGLE\"] = pd.to_numeric(left[COL_ANG], errors=\"coerce\")\n",
    "left[\"PEDTYPE\"]          = pd.to_numeric(left[COL_TYPE], errors=\"coerce\")\n",
    "\n",
    "left_nn  = left[left[\"match_id\"].notna() & left[\"SIMSEC\"].notna()].copy()\n",
    "left_na  = left[left[\"match_id\"].isna()  | left[\"SIMSEC\"].isna()].copy()\n",
    "right_nn = right[right[\"match_id\"].notna() & right[\"SIMSEC\"].notna()].copy().rename(\n",
    "    columns={COL_ANG:\"NeighborAngle\"}\n",
    ")\n",
    "\n",
    "pieces = []\n",
    "if not left_nn.empty:\n",
    "    for mid, lgrp in left_nn.groupby(\"match_id\", sort=False):\n",
    "        rgrp = right_nn[right_nn[\"match_id\"] == mid]\n",
    "        lsorted = lgrp.sort_values(\"SIMSEC\", kind=\"mergesort\").reset_index(drop=True)\n",
    "        if rgrp.empty:\n",
    "            out = lsorted.copy()\n",
    "            out[\"NeighborAngle\"] = np.nan\n",
    "        else:\n",
    "            rsorted = rgrp.sort_values(\"SIMSEC\", kind=\"mergesort\").reset_index(drop=True)\n",
    "            out = pd.merge_asof(\n",
    "                lsorted, rsorted[[\"SIMSEC\",\"NeighborAngle\"]],\n",
    "                on=\"SIMSEC\", direction=\"nearest\", tolerance=TOL_NEIGHBOR_SEC\n",
    "            )\n",
    "        pieces.append(out)\n",
    "\n",
    "merged_nn = pd.concat(pieces, ignore_index=True, sort=False) if pieces else left_nn.copy()\n",
    "if \"NeighborAngle\" not in left_na.columns:\n",
    "    left_na[\"NeighborAngle\"] = np.nan\n",
    "merged = pd.concat([merged_nn, left_na], ignore_index=True, sort=False)\n",
    "\n",
    "# ★修正点1：NEARNEIGHB は自分自身の match_id を採用（別DFからの代入を禁止）\n",
    "merged[\"NEARNEIGHB\"] = merged[\"match_id\"]\n",
    "\n",
    "# --- ラベル付け・相互作用 ---\n",
    "id_to_type = (\n",
    "    df[[COL_ID, COL_TYPE]].dropna().drop_duplicates().set_index(COL_ID)[COL_TYPE].to_dict()\n",
    ")\n",
    "\n",
    "def safe_get_type(x):\n",
    "    try:\n",
    "        return id_to_type.get(float(x))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def label_from_type(p):\n",
    "    if pd.isna(p): return \"Unknown\"\n",
    "    if p in (100, 200): return \"Human\"\n",
    "    if p == 300:        return \"Robot\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "merged[\"NeighborType\"]  = merged[\"NEARNEIGHB\"].map(safe_get_type)\n",
    "merged[\"SelfLabel\"]     = merged[\"PEDTYPE\"].map(label_from_type)\n",
    "merged[\"NeighborLabel\"] = merged[\"NeighborType\"].map(label_from_type)\n",
    "\n",
    "valid = (\n",
    "    (~merged[\"NeighborAngle\"].isna()) &\n",
    "    (~merged[\"NEARNEIGHBDIST\"].isna()) &\n",
    "    (merged[\"NEARNEIGHBDIST\"] < DIST_LIMIT)\n",
    ")\n",
    "\n",
    "ang_diff = (merged[\"ORIENTATIONANGLE\"] - merged[\"NeighborAngle\"]).abs()\n",
    "ang_diff = np.where(ang_diff <= 180, ang_diff, 360 - ang_diff)\n",
    "merged[\"AngleDiff\"] = ang_diff\n",
    "\n",
    "merged[\"InteractionType\"] = np.where(\n",
    "    ~valid, \"Unknown\",\n",
    "    np.where(ang_diff >= PASS_BY_DEG, \"Pass By\", \"Overtake\")\n",
    ")\n",
    "\n",
    "def _norm_label(x):\n",
    "    if x == \"Human\": return \"Human\"\n",
    "    if x == \"Robot\": return \"Robot\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def pair_type_directional(itype, a, b):\n",
    "    \"\"\"\n",
    "    Overtake:   順序付き（Self -> Neighbor） 例: Human-Robot / Robot-Human\n",
    "    Pass By等:  対称（Human-Robot に丸める）\n",
    "    \"\"\"\n",
    "    a = _norm_label(a)\n",
    "    b = _norm_label(b)\n",
    "    if \"Unknown\" in (a, b):\n",
    "        return \"Unknown\"\n",
    "\n",
    "    if itype == \"Overtake\":\n",
    "        # 誰が誰を追い越したかを区別\n",
    "        return f\"{a}-{b}\"\n",
    "\n",
    "    # それ以外は対称扱い\n",
    "    if a == b:\n",
    "        return f\"{a}-{a}\"      # Human-Human / Robot-Robot\n",
    "    return \"Human-Robot\"       # 片側に集約\n",
    "\n",
    "# ← ここが差し替えポイント（InteractionTypeも使う）\n",
    "merged[\"PairType\"] = [\n",
    "    pair_type_directional(it, sl, nl)\n",
    "    for it, sl, nl in zip(merged[\"InteractionType\"], merged[\"SelfLabel\"], merged[\"NeighborLabel\"])\n",
    "]\n",
    "\n",
    "\n",
    "# --- イベント開始点検出 ---\n",
    "merged = merged.sort_values([COL_ID, \"SIMSEC\"]).reset_index(drop=True)\n",
    "merged[\"SelfID\"] = merged[COL_ID]  # 後で AgentPair 生成に使用\n",
    "\n",
    "def to_int_or_nan(x):\n",
    "    try:    return int(float(x))\n",
    "    except Exception: return np.nan\n",
    "\n",
    "merged[\"NEARNEIGHB_norm\"] = merged[\"NEARNEIGHB\"].map(to_int_or_nan)\n",
    "\n",
    "def mark_starts(g):\n",
    "    g = g.copy()\n",
    "    g[\"prev_type\"] = g[\"InteractionType\"].shift(1)\n",
    "    g[\"prev_nbr\"]  = g[\"NEARNEIGHB_norm\"].shift(1)\n",
    "    g[\"prev_t\"]    = g[\"SIMSEC\"].shift(1)\n",
    "    g[\"is_start\"]  = (\n",
    "        (g[\"InteractionType\"] != g[\"prev_type\"]) |\n",
    "        (g[\"NEARNEIGHB_norm\"] != g[\"prev_nbr\"]) |\n",
    "        (g[\"prev_t\"].isna()) |\n",
    "        ((g[\"SIMSEC\"] - g[\"prev_t\"]) > EVENT_GAP_SEC)\n",
    "    )\n",
    "    return g\n",
    "\n",
    "try:\n",
    "    merged = merged.groupby(merged[COL_ID], group_keys=False).apply(mark_starts, include_groups=False)\n",
    "except TypeError:  # pandas < 2.1 互換\n",
    "    merged = merged.groupby(merged[COL_ID], group_keys=False).apply(mark_starts)\n",
    "\n",
    "# --- 開始点（自身座標のみ保持）---\n",
    "def pair_name(a, b):\n",
    "    try:\n",
    "        aa, bb = sorted([int(a), int(b)])\n",
    "        return f\"{aa}-{bb}\"\n",
    "    except Exception:\n",
    "        return f\"{a}-{b}\"\n",
    "\n",
    "ev = merged[\n",
    "    (merged[\"is_start\"]) &\n",
    "    (~merged[\"NEARNEIGHB_norm\"].isna()) &\n",
    "    (merged[\"InteractionType\"] != \"Unknown\") &\n",
    "    (merged[\"PairType\"] != \"Unknown\")\n",
    "].copy()\n",
    "\n",
    "# ★修正点2：AgentPair は merged 由来の SelfID を参照（df 参照ズレを排除）\n",
    "ev[\"AgentPair\"] = [pair_name(a, b) for a, b in zip(merged.loc[ev.index, \"SelfID\"], ev[\"NEARNEIGHB_norm\"])]\n",
    "\n",
    "ev = (\n",
    "    ev[[\"AgentPair\",\"InteractionType\",\"PairType\",\"SIMSEC\",\"SelfX\",\"SelfY\",\"SelfZ\"]]\n",
    "    .rename(columns={\"SIMSEC\":\"StartTime\"})\n",
    "    .sort_values(\"StartTime\")\n",
    ")\n",
    "\n",
    "# --- 時刻クラスタリング（代表＝先頭）---\n",
    "def cluster_times(times, thr):\n",
    "    times = sorted(t for t in times if pd.notna(t))\n",
    "    if not times: return []\n",
    "    clusters, cur = [], [times[0]]\n",
    "    for t in times[1:]:\n",
    "        if (t - cur[-1]) <= thr: cur.append(t)\n",
    "        else: clusters.append(cur); cur = [t]\n",
    "    clusters.append(cur); return clusters\n",
    "\n",
    "ev_key = {(r.AgentPair, r.InteractionType, r.PairType, r.StartTime):\n",
    "          (r.SelfX, r.SelfY, r.SelfZ) for r in ev.itertuples()}\n",
    "\n",
    "records = []\n",
    "if not ev.empty:\n",
    "    for (pair, itype, ptype), g in ev.groupby([\"AgentPair\",\"InteractionType\",\"PairType\"], sort=False):\n",
    "        for cl in cluster_times(g[\"StartTime\"].tolist(), CLUSTER_SEC):\n",
    "            rep_t = cl[0]\n",
    "            sx, sy, sz = ev_key.get((pair, itype, ptype, rep_t), (np.nan,)*3)\n",
    "            records.append({\n",
    "                \"AgentPair\": pair, \"InteractionType\": itype, \"PairType\": ptype,\n",
    "                \"Count\": 1, \"StartTimes\": cl, \"RepTime\": rep_t,\n",
    "                \"SelfX\": sx, \"SelfY\": sy, \"SelfZ\": sz\n",
    "            })\n",
    "\n",
    "# ★空ガード\n",
    "final_df = pd.DataFrame(records, columns=[\n",
    "    \"AgentPair\",\"InteractionType\",\"PairType\",\"Count\",\n",
    "    \"StartTimes\",\"RepTime\",\"SelfX\",\"SelfY\",\"SelfZ\"\n",
    "])\n",
    "if not final_df.empty:\n",
    "    final_df = final_df.sort_values(\n",
    "        [\"AgentPair\",\"InteractionType\",\"PairType\",\"RepTime\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "# --- サマリ（空対応 & 列名固定）---\n",
    "pair_types = [\"Human-Human\", \"Human-Robot\", \"Robot-Human\", \"Robot-Robot\"]\n",
    "interaction_types = [\"Overtake\", \"Pass By\"]\n",
    "\n",
    "if final_df.empty:\n",
    "    summary = pd.DataFrame(\n",
    "        [(pt, it, 0) for pt in pair_types for it in interaction_types],\n",
    "        columns=[\"PairType\",\"InteractionType\",\"Count\"]\n",
    "    )\n",
    "else:\n",
    "    summary = (\n",
    "        final_df.groupby([\"PairType\",\"InteractionType\"])[\"Count\"]\n",
    "                .sum().reset_index()\n",
    "                .set_index([\"PairType\",\"InteractionType\"])\n",
    "                .reindex(pd.MultiIndex.from_product([pair_types, interaction_types]), fill_value=0)\n",
    "                .reset_index()\n",
    "    )\n",
    "    summary.columns = [\"PairType\",\"InteractionType\",\"Count\"]\n",
    "\n",
    "# === 出力（表示のみ）===\n",
    "_show(final_df, \"=== Events（代表点 / 自身座標のみ） ===\", head=15)\n",
    "_show(summary,  \"=== PairType × InteractionType 件数 ===\")\n",
    "_show(ev,       \"=== イベント開始点（Raw / 自身座標のみ） ===\", head=15)\n",
    "\n",
    "# === ここでは保存しない。変数に保持する ===\n",
    "events_final_df  = final_df\n",
    "events_summary   = summary\n",
    "events_raw_points = ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === セル3：イベント回数の各種集計（堅牢版） ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 表示ユーティリティ（displayが無い環境でも出す）\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "def _show(df, title=None):\n",
    "    if title: print(title)\n",
    "    if _display:\n",
    "        _display(df)\n",
    "    else:\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "# 依存オブジェクト確認（ev / merged / cluster_times / CLUSTER_SEC）\n",
    "if 'ev' not in globals() or not isinstance(ev, pd.DataFrame):\n",
    "    raise RuntimeError(\"ev が見つかりません。セル2（イベント検出）を先に実行してください。\")\n",
    "if 'merged' not in globals() or not isinstance(merged, pd.DataFrame):\n",
    "    raise RuntimeError(\"merged が見つかりません。セル2（イベント検出）を先に実行してください。\")\n",
    "\n",
    "# cluster_times / CLUSTER_SEC がなければ最低限を用意\n",
    "if 'CLUSTER_SEC' not in globals():\n",
    "    CLUSTER_SEC = 3.0\n",
    "if 'cluster_times' not in globals():\n",
    "    def cluster_times(times, thr):\n",
    "        times = sorted(t for t in times if pd.notna(t))\n",
    "        if not times: return []\n",
    "        clusters, cur = [], [times[0]]\n",
    "        for t in times[1:]:\n",
    "            if (t - cur[-1]) <= thr: cur.append(t)\n",
    "            else: clusters.append(cur); cur = [t]\n",
    "        clusters.append(cur); return clusters\n",
    "\n",
    "# ---- 便利関数 ----\n",
    "def ped_to_agent_cat(pedtype):\n",
    "    \"\"\"100/200→Human、300→Robot、それ以外→Unknown\"\"\"\n",
    "    try:\n",
    "        v = int(float(pedtype))\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "    if v in (100, 200): return \"Human\"\n",
    "    if v == 300:        return \"Robot\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# 列名ゆらぎ検出（dfが必要）\n",
    "def norm(s: str) -> str:\n",
    "    import unicodedata as ud\n",
    "    s = ud.normalize(\"NFKC\", str(s)).strip().lower()\n",
    "    s = s.replace(\"$\",\"\").replace(\"：\",\":\").replace(\"（\",\"(\").replace(\"）\",\")\")\n",
    "    for ch in [\" \", \"\\t\", \"-\", \"/\", \"\\\\\", \":\"]:\n",
    "        s = s.replace(ch, \"_\")\n",
    "    while \"__\" in s: s = s.replace(\"__\",\"_\")\n",
    "    return s\n",
    "def find_col(df_: pd.DataFrame, candidates) -> str:\n",
    "    nmap = {c: norm(c) for c in df_.columns}\n",
    "    wants = {norm(c) for c in candidates}\n",
    "    for col, coln in nmap.items():\n",
    "        if coln in wants:\n",
    "            return col\n",
    "    return \"\"\n",
    "\n",
    "if 'df' not in globals() or not isinstance(df, pd.DataFrame):\n",
    "    raise RuntimeError(\"df が見つかりません。読み込みセルを先に実行してください。\")\n",
    "\n",
    "# --- Self 情報の復元（ev の index は merged の index継承前提） ---\n",
    "_ev_work = ev.copy()\n",
    "_ev_work[\"SelfID\"]      = merged.loc[_ev_work.index, \"SelfID\"].values\n",
    "_ev_work[\"SelfPEDTYPE\"] = merged.loc[_ev_work.index, \"PEDTYPE\"].values\n",
    "_ev_work[\"SelfCat\"]     = _ev_work[\"SelfPEDTYPE\"].map(ped_to_agent_cat)\n",
    "\n",
    "# --- 代表イベントへ再クラスタ（クラスタ先頭を代表） ---\n",
    "records = []\n",
    "for (pair, itype, ptype), g in _ev_work.groupby([\"AgentPair\",\"InteractionType\",\"PairType\"], sort=False):\n",
    "    times = sorted(g[\"StartTime\"].tolist())\n",
    "    clusters = cluster_times(times, CLUSTER_SEC)\n",
    "    for cl in clusters:\n",
    "        rep_t = cl[0]\n",
    "        gi = g.loc[g[\"StartTime\"] == rep_t].iloc[0]  # 同時刻複数でも先頭\n",
    "        records.append({\n",
    "            \"AgentPair\": pair,\n",
    "            \"InteractionType\": itype,\n",
    "            \"PairType\": ptype,\n",
    "            \"RepTime\": rep_t,\n",
    "            \"SelfID\": gi[\"SelfID\"],\n",
    "            \"SelfPEDTYPE\": gi[\"SelfPEDTYPE\"],\n",
    "            \"SelfCat\": ped_to_agent_cat(gi[\"SelfPEDTYPE\"])\n",
    "        })\n",
    "\n",
    "events = pd.DataFrame.from_records(records)\n",
    "if events.empty:\n",
    "    events = pd.DataFrame(columns=[\n",
    "        \"AgentPair\",\"InteractionType\",\"PairType\",\"RepTime\",\n",
    "        \"SelfID\",\"SelfPEDTYPE\",\"SelfCat\"\n",
    "    ])\n",
    "\n",
    "# ===== ① 全体のイベント回数 =====\n",
    "overall_events_count = len(events)\n",
    "print(f\"【全体のイベント回数】 {overall_events_count}\")\n",
    "\n",
    "# ===== ② PEDTYPE別（Self）イベント回数 =====\n",
    "cat_counts = (\n",
    "    events.query(\"SelfCat in ['Human','Robot']\")\n",
    "          .groupby(\"SelfCat\", as_index=False)\n",
    "          .size()\n",
    "          .rename(columns={\"size\":\"Count\"})\n",
    ")\n",
    "cat_counts[\"SelfCat\"] = pd.Categorical(cat_counts[\"SelfCat\"], [\"Human\",\"Robot\"])\n",
    "cat_counts = cat_counts.sort_values(\"SelfCat\").reset_index(drop=True)\n",
    "print(\"\\n【PEDTYPE別（Self）イベント回数】\")\n",
    "_show(cat_counts)\n",
    "\n",
    "# ===== ③ PEDTYPE別 × 1800秒ビンのイベント回数（ピボット） =====\n",
    "if not events.empty:\n",
    "    events_time = events.copy()\n",
    "    events_time[\"TimeBinStart\"] = (events_time[\"RepTime\"] // 1800 * 1800).astype(float)\n",
    "    time_cat_counts = (\n",
    "        events_time.query(\"SelfCat in ['Human','Robot']\")\n",
    "                   .groupby([\"TimeBinStart\",\"SelfCat\"], as_index=False)\n",
    "                   .size()\n",
    "                   .rename(columns={\"size\":\"Count\"})\n",
    "    )\n",
    "    time_cat_pivot = (\n",
    "        time_cat_counts.pivot(index=\"TimeBinStart\", columns=\"SelfCat\", values=\"Count\")\n",
    "                       .fillna(0).astype(int).sort_index()\n",
    "    )\n",
    "else:\n",
    "    time_cat_counts = pd.DataFrame(columns=[\"TimeBinStart\",\"SelfCat\",\"Count\"])\n",
    "    time_cat_pivot  = pd.DataFrame(columns=[\"Human\",\"Robot\"]).astype(int)\n",
    "print(\"\\n【PEDTYPE別 × 1800秒ビンごとのイベント回数】（ピボット）\")\n",
    "_show(time_cat_pivot)\n",
    "\n",
    "# ===== ④ PEDTYPE別 × STAROUTDECNO ごとのイベント回数 =====\n",
    "COL_T    = find_col(df, [\"SIMSEC\",\"TIME\",\"TIMESTAMP\",\"SEC\"])\n",
    "COL_ID   = find_col(df, [\"$PEDESTRIAN:NO\",\"PEDESTRIAN:NO\",\"PEDESTRIAN_NO\",\"AGENT_ID\",\"ID\",\"AgentID\"])\n",
    "COL_STAR = find_col(df, [\"STAROUTDECNO\",\"STAR_OUT_DEC_NO\",\"STAROUT_DEC_NO\",\"STAROUTDEC_NO\",\"STAROUT_DECNO\",\"STAROUTDEC\"])\n",
    "\n",
    "star_counts = pd.DataFrame(columns=[\"SelfCat\",\"STAROUTDECNO\",\"Count\"])\n",
    "\n",
    "if COL_T and COL_ID and COL_STAR and not events.empty:\n",
    "    tmp = df[[COL_ID, COL_T, COL_STAR]].copy()\n",
    "    tmp = tmp.rename(columns={COL_ID:\"ID\", COL_T:\"SIMSEC\", COL_STAR:\"STAROUTDECNO\"})\n",
    "    tmp[\"ID\"]           = pd.to_numeric(tmp[\"ID\"], errors=\"coerce\")\n",
    "    tmp[\"SIMSEC\"]       = pd.to_numeric(tmp[\"SIMSEC\"], errors=\"coerce\")\n",
    "    tmp[\"STAROUTDECNO\"] = pd.to_numeric(tmp[\"STAROUTDECNO\"], errors=\"coerce\")\n",
    "\n",
    "    star_results = []\n",
    "    for aid, g in tmp.dropna(subset=[\"ID\",\"SIMSEC\"]).sort_values([\"ID\",\"SIMSEC\"]).groupby(\"ID\", sort=False):\n",
    "        g = g[[\"SIMSEC\",\"STAROUTDECNO\"]].copy().reset_index(drop=True)\n",
    "        e_sub = events.loc[events[\"SelfID\"] == aid, [\"RepTime\",\"SelfCat\"]].copy()\n",
    "        if e_sub.empty:\n",
    "            continue\n",
    "        e_sub = e_sub.sort_values(\"RepTime\").reset_index(drop=True)\n",
    "        e_sub_ren = e_sub.rename(columns={\"RepTime\":\"SIMSEC\"})\n",
    "        merged_star = pd.merge_asof(\n",
    "            e_sub_ren.sort_values(\"SIMSEC\"),\n",
    "            g, on=\"SIMSEC\", direction=\"backward\", tolerance=1800.0  # 直前の値を採用（許容広め）\n",
    "        )\n",
    "        merged_star = merged_star.rename(columns={\"SIMSEC\":\"RepTime\"})\n",
    "        merged_star[\"SelfID\"] = aid\n",
    "        star_results.append(merged_star)\n",
    "\n",
    "    if star_results:\n",
    "        star_ev = pd.concat(star_results, ignore_index=True)\n",
    "        star_counts = (\n",
    "            star_ev.query(\"SelfCat in ['Human','Robot'] and STAROUTDECNO.notna()\")\n",
    "                   .groupby([\"SelfCat\",\"STAROUTDECNO\"], as_index=False)\n",
    "                   .size().rename(columns={\"size\":\"Count\"})\n",
    "                   .sort_values([\"SelfCat\",\"STAROUTDECNO\"]).reset_index(drop=True)\n",
    "        )\n",
    "else:\n",
    "    if not COL_STAR:\n",
    "        print(\"\\n⚠️ 注意: STAROUTDECNO 列が見つかりませんでした。④の集計はスキップします。\")\n",
    "    elif events.empty:\n",
    "        print(\"\\n⚠️ 注意: イベントが空のため、④の集計はスキップします。\")\n",
    "\n",
    "print(\"\\n【PEDTYPE別 × STAROUTDECNO ごとのイベント回数】\")\n",
    "_show(star_counts)\n",
    "\n",
    "# === ここでは保存しない：変数として保持 ===\n",
    "events_overall_df        = pd.DataFrame({\"OverallEvents\":[overall_events_count]})\n",
    "events_counts_by_type    = cat_counts\n",
    "events_counts_time_counts= time_cat_counts\n",
    "events_counts_time_pivot = time_cat_pivot\n",
    "events_counts_by_star    = star_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440286e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== セル：Floor別イベント数（表のみ）＆セル別カウント行列（全時間→時間×階層）【画像なし版】 =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- display フォールバック ---\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "def _show(df, title=None, head=None):\n",
    "    if title: print(title)\n",
    "    if df is None:\n",
    "        print(\"(no data)\")\n",
    "        return\n",
    "    if head is not None and hasattr(df, \"head\"):\n",
    "        df = df.head(head)\n",
    "    if _display:\n",
    "        _display(df)\n",
    "    else:\n",
    "        try:\n",
    "            print(df.to_string(index=False))\n",
    "        except Exception:\n",
    "            print(df)\n",
    "\n",
    "# --- 入力（セル2の出力を使用） ---\n",
    "if \"final_df\" in globals() and isinstance(final_df, pd.DataFrame) and not final_df.empty:\n",
    "    src = final_df.copy()                          # 代表イベント（クラスタ済み）\n",
    "    TIME_COL = \"RepTime\"\n",
    "elif \"ev\" in globals() and isinstance(ev, pd.DataFrame) and not ev.empty:\n",
    "    src = ev.copy().rename(columns={\"StartTime\":\"RepTime\"})  # 開始点\n",
    "    TIME_COL = \"RepTime\"\n",
    "else:\n",
    "    raise RuntimeError(\"イベントデータが見つかりません（final_df または ev）。先にセル2を実行してください。\")\n",
    "\n",
    "# --- パラメータ（必要なら変更） ---\n",
    "CELL_SIZE_M     = 1.0     # 1セルのグリッドサイズ[m]\n",
    "FLOOR_DECIMALS  = 1       # SelfZの丸め桁（1=0.1m単位, 0=1m単位 など）\n",
    "TIME_BIN_SEC    = 1800.0  # 時間ビン幅[秒]\n",
    "\n",
    "# --- 必須列チェック ---\n",
    "need = {\"SelfX\",\"SelfY\",\"SelfZ\", TIME_COL}\n",
    "miss = [c for c in need if c not in src.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"必要列が不足しています: {miss}\")\n",
    "\n",
    "# --- 前処理 ---\n",
    "data = src.dropna(subset=[\"SelfX\",\"SelfY\",\"SelfZ\", TIME_COL]).copy()\n",
    "if data.empty:\n",
    "    print(\"⚠️ 有効なイベント行（SelfX/Y/Z と時間）がありません。空の出力を保持します。\")\n",
    "    # 空でも後続セルが落ちないように空の器を用意\n",
    "    floor_counts = pd.DataFrame(columns=[\"FloorZ\",\"Count\"])\n",
    "    x_edges = y_edges = np.array([0,1], dtype=float)\n",
    "    # ヒートマップに相当するセル別カウントの行列（CSV化予定）の格納先\n",
    "    heat_alltime_counts = []  # [{csv_name, counts_df}]\n",
    "    heat_time_counts    = []  # [{csv_name, counts_df}]\n",
    "else:\n",
    "    # 数値化（保険）\n",
    "    for c in [\"SelfX\",\"SelfY\",\"SelfZ\", TIME_COL]:\n",
    "        data[c] = pd.to_numeric(data[c], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[\"SelfX\",\"SelfY\",\"SelfZ\", TIME_COL]).copy()\n",
    "\n",
    "    data[\"FloorZ\"] = data[\"SelfZ\"].round(FLOOR_DECIMALS)\n",
    "    data[\"TimeBinStart\"] = (np.floor_divide(data[TIME_COL].astype(float), TIME_BIN_SEC) * TIME_BIN_SEC).astype(float)\n",
    "\n",
    "    # --- グリッド範囲（全図共通） ---\n",
    "    def get_bounds(x, y, pad_ratio=0.02, min_pad=0.5):\n",
    "        x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)\n",
    "        xmin, xmax = float(np.nanmin(x)), float(np.nanmax(x))\n",
    "        ymin, ymax = float(np.nanmin(y)), float(np.nanmax(y))\n",
    "        # 同一点ばかりのときに幅ゼロにならないようにパディング\n",
    "        xrange = max(xmax - xmin, min_pad)\n",
    "        yrange = max(ymax - ymin, min_pad)\n",
    "        xpad = max(xrange * pad_ratio, CELL_SIZE_M) * 0.5\n",
    "        ypad = max(yrange * pad_ratio, CELL_SIZE_M) * 0.5\n",
    "        return (xmin - xpad, xmax + xpad, ymin - ypad, ymax + ypad)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = get_bounds(data[\"SelfX\"].values, data[\"SelfY\"].values)\n",
    "\n",
    "    # 端を含むエッジ列（数値誤差対策）\n",
    "    eps = 1e-9\n",
    "    x_edges = np.arange(xmin - eps, xmax + CELL_SIZE_M + eps, CELL_SIZE_M)\n",
    "    y_edges = np.arange(ymin - eps, ymax + CELL_SIZE_M + eps, CELL_SIZE_M)\n",
    "    # bins は最低 2 本必要\n",
    "    if len(x_edges) < 2: x_edges = np.array([xmin - 0.5, xmin + 0.5])\n",
    "    if len(y_edges) < 2: y_edges = np.array([ymin - 0.5, ymin + 0.5])\n",
    "\n",
    "    # ===== 1) Event Count by Floor（表のみ表示・保持） =====\n",
    "    floor_counts = (\n",
    "        data.groupby(\"FloorZ\", as_index=False)\n",
    "            .size()\n",
    "            .rename(columns={\"size\": \"Count\"})\n",
    "            .sort_values(\"FloorZ\")\n",
    "            .reset_index(drop=True)\n",
    "    )\n",
    "    print(\"【Event Count by Floor（階層ごとのイベント数）】\")\n",
    "    _show(floor_counts)\n",
    "\n",
    "    # ===== 2) 全時間 × 各階層：セル別カウント行列を作成・保持 =====\n",
    "    heat_alltime_counts = []  # [{csv_name, counts_df}]\n",
    "    for fz, g in data.groupby(\"FloorZ\", sort=True):\n",
    "        df = g.dropna(subset=[\"SelfX\",\"SelfY\"])\n",
    "        if df.empty:\n",
    "            continue\n",
    "        H, xe, ye = np.histogram2d(df[\"SelfX\"], df[\"SelfY\"], bins=[x_edges, y_edges])\n",
    "        counts_df = pd.DataFrame(\n",
    "            H,\n",
    "            index=pd.IntervalIndex.from_breaks(xe).astype(str),\n",
    "            columns=pd.IntervalIndex.from_breaks(ye).astype(str),\n",
    "        )\n",
    "        heat_alltime_counts.append({\n",
    "            \"csv_name\": f\"heatmap_alltime_floor_{str(fz).replace('.','p')}_counts.csv\",\n",
    "            \"counts_df\": counts_df\n",
    "        })\n",
    "\n",
    "    # ===== 3) 時間ごと × 各階層：セル別カウント行列を作成・保持 =====\n",
    "    heat_time_counts = []  # [{csv_name, counts_df}]\n",
    "    for tb, gtb in data.groupby(\"TimeBinStart\", sort=True):\n",
    "        tlabel = int(tb) if pd.notna(tb) else -1\n",
    "        for fz, gf in gtb.groupby(\"FloorZ\", sort=True):\n",
    "            df = gf.dropna(subset=[\"SelfX\",\"SelfY\"])\n",
    "            if df.empty:\n",
    "                continue\n",
    "            H, xe, ye = np.histogram2d(df[\"SelfX\"], df[\"SelfY\"], bins=[x_edges, y_edges])\n",
    "            counts_df = pd.DataFrame(\n",
    "                H,\n",
    "                index=pd.IntervalIndex.from_breaks(xe).astype(str),\n",
    "                columns=pd.IntervalIndex.from_breaks(ye).astype(str),\n",
    "            )\n",
    "            heat_time_counts.append({\n",
    "                \"csv_name\": f\"heatmap_time_{tlabel}_floor_{str(fz).replace('.','p')}_counts.csv\",\n",
    "                \"counts_df\": counts_df\n",
    "            })\n",
    "\n",
    "# === このセルでは保存しない ===\n",
    "# 以下の変数を“最後のセル”で保存に使います：\n",
    "# floor_counts, x_edges, y_edges, CELL_SIZE_M, TIME_BIN_SEC,\n",
    "# heat_alltime_counts (リスト), heat_time_counts (リスト)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# === まとめて保存（最後のセル・整理版） ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import numbers\n",
    "import datetime as dt\n",
    "\n",
    "# ---------- 出力フォルダ ----------\n",
    "OUT_MAIN      = Path(\"outputs\")                # 旅行時間・分散など\n",
    "OUT_STATS     = OUT_MAIN / \"stats\"\n",
    "OUT_INTERM    = OUT_MAIN / \"intermediate\"\n",
    "OUT_EVENTS    = Path(\"outputs_events\")         # イベント回数・イベント結果\n",
    "OUT_HEAT      = Path(\"heatmaps_min\")           # 画像なしヒートマップ（セル別カウントCSV）\n",
    "\n",
    "for p in [OUT_MAIN, OUT_STATS, OUT_INTERM, OUT_EVENTS, OUT_HEAT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- ヘルパ ----------\n",
    "def _df_ok(name): return (name in globals()) and isinstance(globals()[name], pd.DataFrame)\n",
    "def _num_ok(x):   return isinstance(x, numbers.Number)\n",
    "\n",
    "def _save_csv(df, path, index=False):\n",
    "    df.to_csv(path, index=index, encoding=\"utf-8-sig\")\n",
    "    # print(f\"[保存] {path}\")  # 必要ならコメント解除\n",
    "\n",
    "# ==============================================================\n",
    "# 1) 旅行時間系（第一/第二処理の成果）\n",
    "# ==============================================================\n",
    "if _df_ok(\"result\"):\n",
    "    _save_csv(result, OUT_MAIN / \"travel_times_by_ped.csv\")\n",
    "else:\n",
    "    print(\"[注意] result なし：旅行時間（歩行者別）をスキップ。\")\n",
    "\n",
    "if _df_ok(\"by_type\"):\n",
    "    _save_csv(by_type, OUT_MAIN / \"avg_travel_time_by_agent_type.csv\")\n",
    "if _df_ok(\"by_bin\"):\n",
    "    _save_csv(by_bin, OUT_MAIN / \"avg_travel_time_by_agent_and_timebin.csv\")\n",
    "if _df_ok(\"result_with_type\"):\n",
    "    _save_csv(result_with_type, OUT_MAIN / \"travel_times_with_agent_type.csv\")\n",
    "if _df_ok(\"df_sorted\"):\n",
    "    _save_csv(df_sorted, OUT_INTERM / \"df_sorted.csv\")\n",
    "\n",
    "# --- AGENT_TYPE × STAROUTDECNO ---\n",
    "if _df_ok(\"avg_travel_by_agent_star\"):\n",
    "    _save_csv(avg_travel_by_agent_star, OUT_MAIN / \"avg_travel_time_by_agent_star.csv\")\n",
    "if _df_ok(\"result_with_meta\"):\n",
    "    _save_csv(result_with_meta, OUT_MAIN / \"travel_time_with_agent_star_meta.csv\")\n",
    "\n",
    "# --- 元 df を .parsed.csv で（PATH があれば同ディレクトリ） ---\n",
    "if _df_ok(\"df\"):\n",
    "    src = None\n",
    "    try:\n",
    "        if \"PATH\" in globals() and PATH:\n",
    "            src = Path(PATH)\n",
    "    except Exception:\n",
    "        src = None\n",
    "\n",
    "    out_path_df = (src.with_suffix(\".parsed.csv\") if src else OUT_MAIN / \"df.parsed.csv\")\n",
    "    out_path_df.parent.mkdir(parents=True, exist_ok=True)\n",
    "    _save_csv(df, out_path_df)\n",
    "\n",
    "# --- スカラー（overall/total） ---\n",
    "if \"overall\" in globals() and _num_ok(overall):\n",
    "    (OUT_STATS / \"overall_avg_travel_time.txt\").write_text(str(overall), encoding=\"utf-8\")\n",
    "    (OUT_STATS / \"overall_avg_travel_time.json\").write_text(\n",
    "        json.dumps({\"overall_avg_travel_time\": float(overall)}), encoding=\"utf-8\"\n",
    "    )\n",
    "else:\n",
    "    print(\"[注意] overall なし/数値でない：全体平均TRAVEL_TIMEをスキップ。\")\n",
    "\n",
    "if \"total\" in globals() and _num_ok(total):\n",
    "    (OUT_STATS / \"total_travel_time_sum.txt\").write_text(str(total), encoding=\"utf-8\")\n",
    "    (OUT_STATS / \"total_travel_time_sum.json\").write_text(\n",
    "        json.dumps({\"total_travel_time_sum\": float(total)}), encoding=\"utf-8\"\n",
    "    )\n",
    "else:\n",
    "    print(\"[注意] total なし/数値でない：TRAVEL_TIME総和をスキップ。\")\n",
    "\n",
    "# ==============================================================\n",
    "# 2) 速度分散の集計（overall_df / by_type / by_type_time / by_type_star）\n",
    "# ==============================================================\n",
    "if _df_ok(\"overall_df\"):\n",
    "    _save_csv(overall_df, OUT_MAIN / \"mean_var_overall.csv\")\n",
    "else:\n",
    "    print(\"[注意] overall_df なし：分散の全体平均をスキップ。\")\n",
    "if _df_ok(\"by_type\"):\n",
    "    _save_csv(by_type, OUT_MAIN / \"mean_var_by_type.csv\")\n",
    "if _df_ok(\"by_type_time\"):\n",
    "    _save_csv(by_type_time, OUT_MAIN / \"mean_var_by_type_timebin.csv\")\n",
    "if _df_ok(\"by_type_star\"):\n",
    "    _save_csv(by_type_star, OUT_MAIN / \"mean_var_by_type_star.csv\")\n",
    "\n",
    "# メタ（速度分散）\n",
    "mv_meta = {}\n",
    "if \"BIN\" in globals() and _num_ok(BIN): mv_meta[\"BIN_seconds\"] = int(BIN)\n",
    "if \"var_col\" in globals():              mv_meta[\"var_col\"] = var_col\n",
    "if \"time_col\" in globals():             mv_meta[\"time_col\"] = time_col\n",
    "mv_meta[\"generated_at\"] = dt.datetime.now().isoformat()\n",
    "(OUT_STATS / \"mean_var_meta.json\").write_text(json.dumps(mv_meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ==============================================================\n",
    "# 3) 速度分散：第二処理（ラベル件数）\n",
    "# ==============================================================\n",
    "if _df_ok(\"level_counts\"):\n",
    "    _save_csv(level_counts, OUT_MAIN / \"expvelvar_level_summary.csv\")\n",
    "else:\n",
    "    print(\"[注意] level_counts なし：レベル総数をスキップ。\")\n",
    "if _df_ok(\"level_by_ped\"):\n",
    "    _save_csv(level_by_ped, OUT_MAIN / \"expvelvar_level_by_ped.csv\")\n",
    "\n",
    "# ==============================================================\n",
    "# 4) イベント回数（セル3）\n",
    "# ==============================================================\n",
    "if _df_ok(\"events_overall_df\"):\n",
    "    _save_csv(events_overall_df, OUT_EVENTS / \"overall_event_count.csv\")\n",
    "else:\n",
    "    print(\"[注意] events_overall_df なし：イベント総数CSVをスキップ。\")\n",
    "\n",
    "if _df_ok(\"events_counts_by_type\"):\n",
    "    _save_csv(events_counts_by_type, OUT_EVENTS / \"event_counts_by_PEDTYPE.csv\")\n",
    "if _df_ok(\"events_counts_time_counts\"):\n",
    "    _save_csv(events_counts_time_counts, OUT_EVENTS / \"event_counts_by_PEDTYPE_timebin1800.csv\")\n",
    "if _df_ok(\"events_counts_time_pivot\"):\n",
    "    # ピボットは index 付きで保存（行ラベル＝TimeBinStart）\n",
    "    events_counts_time_pivot.to_csv(OUT_EVENTS / \"event_counts_by_PEDTYPE_timebin1800_pivot.csv\",\n",
    "                                    encoding=\"utf-8-sig\")\n",
    "if _df_ok(\"events_counts_by_star\"):\n",
    "    _save_csv(events_counts_by_star, OUT_EVENTS / \"event_counts_by_PEDTYPE_STAROUTDECNO.csv\")\n",
    "\n",
    "# メタ（イベント回数セル3）\n",
    "ev_meta = {\"time_bin_seconds\": 1800}\n",
    "if \"CLUSTER_SEC\" in globals() and _num_ok(CLUSTER_SEC): ev_meta[\"CLUSTER_SEC\"] = float(CLUSTER_SEC)\n",
    "(OUT_EVENTS / \"events_meta.json\").write_text(json.dumps(ev_meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ==============================================================\n",
    "# 5) イベント検出の最終/中間（final_df/summary/raw）\n",
    "# ==============================================================\n",
    "if _df_ok(\"events_final_df\"):\n",
    "    _save_csv(events_final_df, OUT_EVENTS / \"final_events_self_only.csv\")\n",
    "else:\n",
    "    print(\"[注意] events_final_df なし：final をスキップ。\")\n",
    "if _df_ok(\"events_summary\"):\n",
    "    _save_csv(events_summary, OUT_EVENTS / \"pairtype_summary.csv\")\n",
    "if _df_ok(\"events_raw_points\"):\n",
    "    _save_csv(events_raw_points, OUT_EVENTS / \"event_points_self_only.csv\")\n",
    "\n",
    "# 閾値メタ（イベント検出セル）\n",
    "ev_thresh = {}\n",
    "for k in [\"TOL_NEIGHBOR_SEC\",\"EVENT_GAP_SEC\",\"CLUSTER_SEC\",\"DIST_LIMIT\",\"PASS_BY_DEG\"]:\n",
    "    if k in globals():\n",
    "        ev_thresh[k] = globals()[k]\n",
    "(OUT_EVENTS / \"events_thresholds.json\").write_text(json.dumps(ev_thresh, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ==============================================================\n",
    "# 6) 画像なしヒートマップ（セル別カウントCSVのみ）\n",
    "# ==============================================================\n",
    "if _df_ok(\"floor_counts\") and not floor_counts.empty:\n",
    "    _save_csv(floor_counts, OUT_HEAT / \"event_count_by_floor.csv\")\n",
    "else:\n",
    "    print(\"[注意] floor_counts なし/空：階層別イベント数CSVをスキップ。\")\n",
    "\n",
    "if \"heat_alltime_counts\" in globals():\n",
    "    for spec in heat_alltime_counts:\n",
    "        _save_csv(spec[\"counts_df\"], OUT_HEAT / spec[\"csv_name\"])\n",
    "else:\n",
    "    print(\"[注意] heat_alltime_counts 未定義：全時間カウントCSVをスキップ。\")\n",
    "\n",
    "if \"heat_time_counts\" in globals():\n",
    "    for spec in heat_time_counts:\n",
    "        _save_csv(spec[\"counts_df\"], OUT_HEAT / spec[\"csv_name\"])\n",
    "else:\n",
    "    print(\"[注意] heat_time_counts 未定義：時間×階層カウントCSVをスキップ。\")\n",
    "\n",
    "# メタ（グリッド）\n",
    "grid_meta = {}\n",
    "if \"CELL_SIZE_M\"  in globals(): grid_meta[\"CELL_SIZE_M\"]  = float(CELL_SIZE_M)\n",
    "if \"TIME_BIN_SEC\" in globals(): grid_meta[\"TIME_BIN_SEC\"] = float(TIME_BIN_SEC)\n",
    "if \"x_edges\"      in globals(): grid_meta[\"X_bins\"]       = int(len(x_edges)-1)\n",
    "if \"y_edges\"      in globals(): grid_meta[\"Y_bins\"]       = int(len(y_edges)-1)\n",
    "(OUT_HEAT / \"grid_meta.json\").write_text(json.dumps(grid_meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ==============================================================\n",
    "# 7) 終了メッセージ\n",
    "# ==============================================================\n",
    "print(\"\\n✅ 保存完了\")\n",
    "print(\" -\", OUT_MAIN.resolve())\n",
    "print(\" -\", OUT_EVENTS.resolve())\n",
    "print(\" -\", OUT_HEAT.resolve())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f976a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 最後のセル：指定シートのみを1つのExcelに保存 ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json, numbers, datetime as dt\n",
    "import re\n",
    "\n",
    "# 出力先とファイル名\n",
    "OUT_DIR = Path(\"outputs_all_in_one_1111デフォルト\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ts = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "xlsx_path = OUT_DIR / f\"analysis_outputs_{ts}.xlsx\"\n",
    "\n",
    "def _df_ok(name):\n",
    "    return (name in globals()) and isinstance(globals()[name], pd.DataFrame)\n",
    "\n",
    "def _num_ok(x):\n",
    "    return isinstance(x, numbers.Number)\n",
    "\n",
    "def _to_df(obj, key_name=\"key\", val_name=\"value\"):\n",
    "    \"\"\"dict/数値などを2列DataFrameへ\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return pd.DataFrame([(k, obj[k]) for k in obj], columns=[key_name, val_name])\n",
    "    if _num_ok(obj):\n",
    "        return pd.DataFrame({val_name:[obj]})\n",
    "    return pd.DataFrame({val_name:[json.dumps(obj, ensure_ascii=False)]})\n",
    "\n",
    "# シート名を安全化（31文字制限・禁止文字・重複対策）\n",
    "def _safe_sheet_name(name, used):\n",
    "    # 禁止文字: : \\ / ? * [ ]\n",
    "    name = re.sub(r'[:\\\\/\\?\\*\\[\\]]', '_', str(name))\n",
    "    name = name.strip().strip(\"'\")  # 先頭/末尾の ' は避ける\n",
    "    if not name:\n",
    "        name = \"Sheet\"\n",
    "    # 既に使われていないかチェック\n",
    "    base = name[:31]\n",
    "    if base not in used:\n",
    "        used.add(base)\n",
    "        return base\n",
    "    # 衝突したら連番付与\n",
    "    for i in range(1, 10000):\n",
    "        suffix = f\"_{i}\"\n",
    "        cand = (base[:31-len(suffix)]) + suffix\n",
    "        if cand not in used:\n",
    "            used.add(cand)\n",
    "            return cand\n",
    "    # 異常系：最後の手段\n",
    "    fallback = f\"Sheet_{len(used)+1}\"\n",
    "    used.add(fallback)\n",
    "    return fallback\n",
    "\n",
    "# ここに「シート名 → DataFrame」を順次積む\n",
    "sheets = []\n",
    "\n",
    "# ===================== ここから出力対象のみ =====================\n",
    "# ★ events_raw_points → \"events_raw_points\"\n",
    "if _df_ok(\"events_raw_points\"):\n",
    "    sheets.append((\"events_raw_points\", events_raw_points))\n",
    "\n",
    "# ★ events_pairtype_summary → \"events_pairtype_summary\"\n",
    "#    （変数名は events_summary を想定）\n",
    "if _df_ok(\"events_summary\"):\n",
    "    sheets.append((\"events_pairtype_summary\", events_summary))\n",
    "\n",
    "# ★ events_by_PEDTYPE_STAROUTDECNO → \"events_by_PEDTYPE_STAROUTDECNO\"\n",
    "#    （変数名は events_counts_by_star を想定）\n",
    "if _df_ok(\"events_counts_by_star\"):\n",
    "    sheets.append((\"events_by_PEDTYPE_STAROUTDECNO\", events_counts_by_star))\n",
    "\n",
    "# ★ events_by_PEDTYPE → \"events_by_PEDTYPE\"\n",
    "#    （変数名は events_counts_by_type を想定）\n",
    "if _df_ok(\"events_counts_by_type\"):\n",
    "    sheets.append((\"events_by_PEDTYPE\", events_counts_by_type))\n",
    "# ===================== ここまで =====================\n",
    "\n",
    "# ---------------- 以下はすべて無効化 ----------------\n",
    "# if _df_ok(\"result\"):                sheets.append((\"travel_times_by_ped\", result))\n",
    "# if _df_ok(\"by_bin\"):                sheets.append((\"avg_travel_time_by_agent_timebin\", by_bin))\n",
    "# if _df_ok(\"result_with_type\"):      sheets.append((\"travel_times_with_agent_type\", result_with_type))\n",
    "# if _df_ok(\"df_sorted\"):             sheets.append((\"df_sorted\", df_sorted))\n",
    "# if _df_ok(\"avg_travel_by_agent_star\"): sheets.append((\"avg_travel_time_by_agent_star\", avg_travel_by_agent_star))\n",
    "# if _df_ok(\"result_with_meta\"):      sheets.append((\"agent_star_meta_join\", result_with_meta))\n",
    "# if \"overall\" in globals() and _num_ok(overall): sheets.append((\"overall_avg_travel_time\", _to_df(overall, val_name=\"overall\")))\n",
    "# if \"total\"   in globals() and _num_ok(total):   sheets.append((\"total_travel_time_sum\", _to_df(total,   val_name=\"total\")))\n",
    "# if _df_ok(\"by_type\"):\n",
    "#     cols = set(map(str, by_type.columns))\n",
    "#     if \"TRAVEL_TIME\" in cols:\n",
    "#         sheets.append((\"avg_travel_time_by_agent_type\", by_type))\n",
    "#     if \"MEAN_VAR\" in cols:\n",
    "#         sheets.append((\"mean_var_by_type\", by_type))\n",
    "# if _df_ok(\"episodes\"):      sheets.append((\"expvelvar_episodes\", episodes))\n",
    "# if _df_ok(\"count_table\"):   sheets.append((\"expvelvar_counts_by_ped\", count_table))\n",
    "# if _df_ok(\"abnormal\"):      sheets.append((\"expvelvar_abnormal_only\", abnormal))\n",
    "# if _df_ok(\"overall_df\"):    sheets.append((\"mean_var_overall\", overall_df))\n",
    "# if _df_ok(\"by_type_time\"):  sheets.append((\"mean_var_by_type_timebin\", by_type_time))\n",
    "# if _df_ok(\"by_type_star\"):  sheets.append((\"mean_var_by_type_star\", by_type_star))\n",
    "# mv_meta = {}\n",
    "# if \"BIN\" in globals() and _num_ok(BIN): mv_meta[\"BIN_seconds\"] = int(BIN)\n",
    "# if \"var_col\" in globals():               mv_meta[\"var_col\"] = var_col\n",
    "# if \"time_col\" in globals():              mv_meta[\"time_col\"] = time_col\n",
    "# mv_meta[\"generated_at\"] = dt.datetime.now().isoformat()\n",
    "# sheets.append((\"mean_var_meta\", _to_df(mv_meta)))\n",
    "if _df_ok(\"events_overall_df\"):          sheets.append((\"events_overall_count\", events_overall_df))\n",
    "# if _df_ok(\"events_counts_time_counts\"):  sheets.append((\"events_by_PEDTYPE_timebin1800\", events_counts_time_counts))\n",
    "# if _df_ok(\"events_counts_time_pivot\"):   sheets.append((\"events_by_PEDTYPE_timebin_pivot\", events_counts_time_pivot))\n",
    "# ev_meta = {\"time_bin_seconds\": 1800}\n",
    "# if \"CLUSTER_SEC\" in globals() and _num_ok(CLUSTER_SEC): ev_meta[\"CLUSTER_SEC\"] = float(CLUSTER_SEC)\n",
    "# sheets.append((\"events_meta\", _to_df(ev_meta)))\n",
    "# if _df_ok(\"events_final_df\"):   sheets.append((\"events_final_self_only\", events_final_df))\n",
    "# ev_thr = {}\n",
    "# for k in [\"TOL_NEIGHBOR_SEC\",\"EVENT_GAP_SEC\",\"CLUSTER_SEC\",\"DIST_LIMIT\",\"PASS_BY_DEG\"]:\n",
    "#     if k in globals(): ev_thr[k] = globals()[k]\n",
    "# sheets.append((\"events_thresholds\", _to_df(ev_thr)))\n",
    "# 画像・ヒートマップ系、raw_df などもすべて無効化\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# ===== 書き込み =====\n",
    "used_names = set()\n",
    "\n",
    "def _open_writer(path):\n",
    "    # 1) xlsxwriter → 2) openpyxl → 3) エンジン指定なし（pandasの既定）\n",
    "    for eng in (\"xlsxwriter\", \"openpyxl\", None):\n",
    "        try:\n",
    "            return pd.ExcelWriter(path, engine=eng) if eng else pd.ExcelWriter(path)\n",
    "        except ModuleNotFoundError:\n",
    "            continue\n",
    "    raise ModuleNotFoundError(\"Neither 'xlsxwriter' nor 'openpyxl' is installed.\")\n",
    "\n",
    "with _open_writer(xlsx_path) as w:\n",
    "    for name, df in sheets:\n",
    "        try:\n",
    "            sheet = _safe_sheet_name(name, used_names)\n",
    "            df.to_excel(w, sheet_name=sheet, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"[警告] シート {name} の書き込みに失敗: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
